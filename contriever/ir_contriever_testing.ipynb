{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e6ce37",
   "metadata": {},
   "source": [
    "Notes: I have tested the retriever using previously generated embedding files and questions extracted from the gpt-3 generated QA pairs. I noticed that the contriever model works best on section embeddings and worst on sentence embeddings.\n",
    "The model does retrieve proper sections - tested using top 5 sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ef8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01462e52",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\ninstall() got an unexpected keyword argument 'show_locals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1110\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:42\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     33\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:83\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__ \u001b[39mas\u001b[39;00m accelerate_version\n\u001b[1;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_model, infer_auto_device_map, init_empty_weights\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/accelerate/__init__.py:34\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m is_rich_available():\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m rich\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/accelerate/utils/rich.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mrich\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraceback\u001b[39;00m \u001b[39mimport\u001b[39;00m install\n\u001b[0;32m---> 21\u001b[0m     install(show_locals\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: install() got an unexpected keyword argument 'show_locals'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mfacebook/contriever-msmarco\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mfacebook/contriever-msmarco\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    460\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m--> 463\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_model_mapping)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:359\u001b[0m, in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[0;32m--> 359\u001b[0m     supported_models \u001b[39m=\u001b[39m model_mapping[\u001b[39mtype\u001b[39;49m(config)]\n\u001b[1;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(supported_models, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    361\u001b[0m         \u001b[39mreturn\u001b[39;00m supported_models\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:591\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m model_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping:\n\u001b[1;32m    590\u001b[0m     model_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 591\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_attr_from_module(model_type, model_name)\n\u001b[1;32m    593\u001b[0m \u001b[39m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m model_types \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_mapping\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v \u001b[39m==\u001b[39m key\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:605\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules:\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules[module_name] \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtransformers.models\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m \u001b[39mreturn\u001b[39;00m getattribute_from_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules[module_name], attr)\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:554\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(attr, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m attr)\n\u001b[0;32m--> 554\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39;49m(module, attr):\n\u001b[1;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    556\u001b[0m \u001b[39m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1100\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1099\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1100\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1101\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1102\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/transformers/utils/import_utils.py:1112\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1113\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1115\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\ninstall() got an unexpected keyword argument 'show_locals'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "model = AutoModel.from_pretrained('facebook/contriever-msmarco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01ad4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ee1874",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"paragraphs.json\")\n",
    "para_data = json.load(fp)\n",
    "\n",
    "fsec = open(\"sections.json\")\n",
    "section_data = json.load(fsec)\n",
    "\n",
    "fs = open(\"sentences.json\")\n",
    "sentence_data = json.load(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7bf452",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_section_embeddings = np.load('section_embeddings.npy')\n",
    "saved_para_embeddings = np.load('paragraph_embeddings.npy')\n",
    "saved_sentence_embeddings = np.load('sentence_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3ee459",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_tensors = torch.from_numpy(saved_section_embeddings)\n",
    "para_tensors = torch.from_numpy(saved_para_embeddings)\n",
    "sentence_tensors = torch.from_numpy(saved_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9279f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "2078\n",
      "4243\n"
     ]
    }
   ],
   "source": [
    "print(len(section_data))\n",
    "print(len(para_data))\n",
    "print(len(sentence_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38d81d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the data\n",
    "\n",
    "def clean(text):\n",
    "    new_text = re.sub('\\n', '', text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6a042",
   "metadata": {},
   "source": [
    "Extracting questions from the gpt-3 sections data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af20f85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of section data: 144\n"
     ]
    }
   ],
   "source": [
    "# loading the gpt-3 data\n",
    "\n",
    "fq = open(\"../data-generator/gpt-3/GPT-3_section_level.json\")\n",
    "gpt3_section_data = json.load(fq)\n",
    "print(\"length of section data:\", len(gpt3_section_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7091c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with section number as key and question as value\n",
    "\n",
    "qc_section_dict = {}\n",
    "for i in range(len(gpt3_section_data)):\n",
    "    clean_question = clean(gpt3_section_data[i]['questions'][4:])\n",
    "    qc_section_dict[i] = clean_question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d0599",
   "metadata": {},
   "source": [
    "Retrieving most relevant section, paragraph, and sentence from saved embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d3b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the section with highest dot product with the question\n",
    "\n",
    "def section_retriever(embedded_question):\n",
    "    section_scores = {}\n",
    "    for i in range(len(section_tensors)):\n",
    "        score = embedded_question[0]@section_tensors[i]\n",
    "        section_scores[i] = score\n",
    "    \n",
    "    highest_score = max(section_scores, key=section_scores.get)\n",
    "    return highest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c087a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the paragraph with highest dot product with the question\n",
    "\n",
    "def paragraph_retriever(embedded_question):\n",
    "    para_scores = {}\n",
    "    for i in range(len(para_tensors)):\n",
    "        score = embedded_question[0]@para_tensors[i]\n",
    "        para_scores[i] = score\n",
    "    \n",
    "    highest_score = max(para_scores, key=para_scores.get)\n",
    "    return highest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b4cf0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves the sentence with highest dot product with the question\n",
    "\n",
    "def sentence_retriever(embedded_question):\n",
    "    sentence_scores = {}\n",
    "    for i in range(len(sentence_tensors)):\n",
    "        score = embedded_question[0]@sentence_tensors[i]\n",
    "        sentence_scores[i] = score\n",
    "    \n",
    "    highest_score = max(sentence_scores, key=sentence_scores.get)\n",
    "    return highest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c2d5888",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating a dataframe with highest section/paragraph/sentence level scores for all questions\n",
    "\n",
    "cols = ['Question', 'GPT-3 Section', 'Section', 'Paragraph', 'Sentence']\n",
    "score_data = []\n",
    "\n",
    "for j in qc_section_dict:\n",
    "    question = qc_section_dict[j]\n",
    "    \n",
    "    #embed the question\n",
    "    tokenized_question = tokenizer(question, padding=True, truncation=True, return_tensors='pt')\n",
    "    output_question = model(**tokenized_question)\n",
    "    embeddings_question = mean_pooling(output_question[0], tokenized_question['attention_mask'])\n",
    "    \n",
    "    # retrieve section/para/sentence\n",
    "    section_no = section_retriever(embeddings_question)\n",
    "    paragraph_no = paragraph_retriever(embeddings_question)\n",
    "    sentence_no = sentence_retriever(embeddings_question)\n",
    "    \n",
    "    # store the question no., question, and scores in a dataframe\n",
    "    row = [question, j, section_no, paragraph_no, sentence_no]\n",
    "    score_data.append(row)\n",
    "\n",
    "score_df = pd.DataFrame(score_data, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3935e879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>GPT-3 Section</th>\n",
       "      <th>Section</th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the design process for a digital FSM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is it important to design digital systems ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>845</td>\n",
       "      <td>1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a Gray code?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does a three-bit gray code counter work?</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is it possible to create a counter with fewer ...</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>What is the meaning of the term \"universal com...</td>\n",
       "      <td>139</td>\n",
       "      <td>60</td>\n",
       "      <td>1687</td>\n",
       "      <td>3636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>What is the overflow condition for unsigned ad...</td>\n",
       "      <td>140</td>\n",
       "      <td>73</td>\n",
       "      <td>1398</td>\n",
       "      <td>3046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Why is the converse of an implication not alwa...</td>\n",
       "      <td>141</td>\n",
       "      <td>81</td>\n",
       "      <td>1571</td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-Why is it important to know when an addition ...</td>\n",
       "      <td>142</td>\n",
       "      <td>53</td>\n",
       "      <td>1397</td>\n",
       "      <td>4124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>If A and B are both positive and S is negative...</td>\n",
       "      <td>143</td>\n",
       "      <td>81</td>\n",
       "      <td>1402</td>\n",
       "      <td>3050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question  GPT-3 Section  \\\n",
       "0    How does the design process for a digital FSM ...              0   \n",
       "1    Why is it important to design digital systems ...              1   \n",
       "2                                 What is a Gray code?              2   \n",
       "3         How does a three-bit gray code counter work?              3   \n",
       "4    Is it possible to create a counter with fewer ...              4   \n",
       "..                                                 ...            ...   \n",
       "139  What is the meaning of the term \"universal com...            139   \n",
       "140  What is the overflow condition for unsigned ad...            140   \n",
       "141  Why is the converse of an implication not alwa...            141   \n",
       "142  -Why is it important to know when an addition ...            142   \n",
       "143  If A and B are both positive and S is negative...            143   \n",
       "\n",
       "     Section  Paragraph  Sentence  \n",
       "0         28        371         1  \n",
       "1          1        845      1883  \n",
       "2          2         12      2600  \n",
       "3          2        124       225  \n",
       "4         41         30        59  \n",
       "..       ...        ...       ...  \n",
       "139       60       1687      3636  \n",
       "140       73       1398      3046  \n",
       "141       81       1571      4119  \n",
       "142       53       1397      4124  \n",
       "143       81       1402      3050  \n",
       "\n",
       "[144 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29cf475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the section level retrievals not matching with GPT-3 data\n",
    "\n",
    "sections_not_match = score_df.loc[~(score_df['GPT-3 Section'] == score_df['Section'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "892968e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections_not_match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd432ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question         Is it possible to create a counter with fewer ...\n",
      "GPT-3 Section                                                    4\n",
      "Section                                                         41\n",
      "Paragraph                                                       30\n",
      "Sentence                                                        59\n",
      "Name: 4, dtype: object\n",
      "-------------\n",
      "Is it possible to create a counter with fewer than 3 flip-flops?\n",
      "-------------\n",
      "{Example: A Color Sequencer}\n",
      "\n",
      "\n",
      "Early graphics systems used a three-bit red-green-blue (RGB) \n",
      "encoding for colors.  The color mapping for such a system is shown to\n",
      "the right.\n",
      "\n",
      "Imagine that you are charged with creating a counter to drive a light\n",
      "through a sequence of colors.  The light takes an RGB input as just\n",
      "described, and the desired pattern is\n",
      "\n",
      "{off (black)     yellow     violet     green     blue}\n",
      "\n",
      "You immediately recognize that you merely need a counter with five\n",
      "states.  How many flip-flops will we need?  At least three, since\n",
      "_2 (5)=3.  Given that we need three flip-flops, \n",
      "and that the colors we need to produce as\n",
      "\n",
      "\n",
      "{c|l}\n",
      "RGB& color \n",
      "000& black\n",
      "001& blue\n",
      "010& green\n",
      "011& cyan\n",
      "100& red\n",
      "101& violet\n",
      "110& yellow\n",
      "111& white\n",
      "\n",
      "\n",
      "\n",
      "outputs are all unique\n",
      "bit patterns, we can again choose to use the counter's internal \n",
      "state directly as our output values.\n",
      "\n",
      "\n",
      "A fully-specified transition diagram for our color sequencer\n",
      "appears to the right.  The states again form a loop,\n",
      "and are marked with the internal state value S_2S_1S_0 \n",
      "and the output RGB.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "As before, we can use the transition diagram to fill in K-maps for the \n",
      "next-state values S_2^+, S_1^+, and S_0^+, as shown to the right.\n",
      "For each of the three states not included in our transition diagram,\n",
      "we have inserted x's\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "into the K-maps to indicate ``don't care.'' \n",
      "As you know, we can treat each x as either a 0 or a 1, whichever\n",
      "produces better results (where ``better'' usually means simpler \n",
      "equations).  The terms that we have chosen for our algebraic \n",
      "equations are illustrated in the K-maps.  The x's within the ellipses\n",
      "become 1s in the implementation, and the x's outside of the ellipses\n",
      "become 0s.\n",
      "\n",
      "\n",
      "For our next-state logic, we obtain:\n",
      "{eqnarray*}\n",
      "S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}} \n",
      "S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}} \n",
      "S_0^+ &=& S_1\n",
      "{eqnarray*}\n",
      "\n",
      "Again our equations for S_2^+ and S_1^+ share a common term,\n",
      "which becomes a single AND gate in the implementation shown to the\n",
      "right.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------\n",
      "SECTION: {Ripple Counters}\n",
      "\n",
      "A second class of\n",
      "counter drives some of its flip-flops with a clock signal and feeds\n",
      "flip-flop outputs into the clock inputs of its remaining flip-flops,\n",
      "possibly through additional logic.  Such a counter is called a {\n",
      "ripple counter}, because the effect of a clock edge ripples through\n",
      "the flip-flops.  The delay inherent to the ripple effect, along with\n",
      "the complexity of ensuring that timing issues do not render the design\n",
      "unreliable, are the major drawbacks of ripple counters.  Compared with\n",
      "synchronous counters, however, ripple counters consume less energy,\n",
      "and are sometimes used for devices with restricted energy supplies.\n",
      "\n",
      "\n",
      "General ripple counters\n",
      "can be tricky because of timing issues, but certain types are easy.\n",
      "\n",
      "Consider the design of binary ripple counter.  The state diagram for \n",
      "a {3-bit} binary counter is replicated to the right.\n",
      "Looking\n",
      "at the states, notice that the least-significant bit alternates with\n",
      "each state, while higher bits flip whenever the next smaller bit (to\n",
      "the right) transitions from one to zero.  To take advantage of these\n",
      "properties, we use positive edge-triggered D flip-flops with\n",
      "their complemented () outputs wired back to their inputs.\n",
      "The clock input is fed only into the first\n",
      "flip-flop, and the complemented output of each flip-flop is also\n",
      "connected to the clock of the next.\n",
      "\n",
      "\n",
      "{file=part3/figs/lec16-9.eps,width=1.5in}\n",
      "\n",
      "\n",
      "\n",
      "An implementation of a {4-bit} binary ripple counter appears to the\n",
      "right.\n",
      "The order of bits in the figure matches the order used for our synchronous\n",
      "binary counters: least significant on the left, most significant on the\n",
      "right.\n",
      "As you can see from the figure, the technique generalizes to arbitrarily \n",
      "large binary ripple coun-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ters, but the\n",
      "time required for the outputs to settle after a clock edge scales with\n",
      "the number of flip-flops in the counter.  On the other hand, an\n",
      "average of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n",
      "), which reduces the power requirements.{Recall that\n",
      "flip-flops record the clock state internally.  The logical activity\n",
      "required to record such state consumes energy.}\n",
      "\n",
      "\n",
      "\n",
      "Beginning with the state 0000, at the rising clock edge, the left (S_0)\n",
      "flip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\n",
      "falling clock edge and does nothing, leaving the counter in\n",
      "state 0001.  When the next rising clock edge arrives, the left\n",
      "flip-flop toggles back to 0, which the second flip-flop sees as a\n",
      "rising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\n",
      "sees the second flip-flop's change as a falling edge and does nothing,\n",
      "and the state settles as 0010.  We leave verification of the remainder \n",
      "of the cycle as an exercise.\n",
      "\n",
      "\n",
      "-------------\n",
      "PARAGRAPH: You immediately recognize that you merely need a counter with five\n",
      "states.  How many flip-flops will we need?  At least three, since\n",
      "_2 (5)=3.  Given that we need three flip-flops, \n",
      "and that the colors we need to produce as\n",
      "-------------\n",
      "SENTENCE:    Given that we need three flip-flops, \n",
      "and that the colors we need to produce as\n",
      "\n",
      "\n",
      "{c|l}\n",
      "RGB& color \n",
      "000& black\n",
      "001& blue\n",
      "010& green\n",
      "011& cyan\n",
      "100& red\n",
      "101& violet\n",
      "110& yellow\n",
      "111& white\n",
      "\n",
      "\n",
      "\n",
      "outputs are all unique\n",
      "bit patterns, we can again choose to use the counter's internal \n",
      "state directly as our output values.\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "print(score_df.loc[k])\n",
    "print(\"-------------\")\n",
    "print(score_df.loc[k]['Question'])\n",
    "print(\"-------------\")\n",
    "print(gpt3_section_data[score_df.loc[k]['GPT-3 Section']]['positive_ctxs']['text'])\n",
    "print(\"-------------\")\n",
    "print(\"SECTION:\", section_data[str(score_df.loc[k]['Section'])])\n",
    "print(\"-------------\")\n",
    "print(\"PARAGRAPH:\", para_data[str(score_df.loc[k]['Paragraph'])])\n",
    "print(\"-------------\")\n",
    "print(\"SENTENCE: \", sentence_data[str(score_df.loc[k]['Sentence'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61ec6c",
   "metadata": {},
   "source": [
    "Retrieving the top 5 sections based on a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a715bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs an array of dot product of the question against all the embeddings\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def section_retriever(embedded_question):\n",
    "    section_scores = {}\n",
    "    for i in range(len(section_tensors)):\n",
    "        score = embedded_question[0]@section_tensors[i]\n",
    "        section_scores[i] = score\n",
    "    \n",
    "    res = dict(sorted(section_scores.items(), key = itemgetter(1), reverse = True)[:5])\n",
    "    highest_score = max(section_scores, key=section_scores.get)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8db79c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it possible to create a counter with fewer than 3 flip-flops?\n"
     ]
    }
   ],
   "source": [
    "question = qc_section_dict[4]\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e944bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the question\n",
    "tokenized_question = tokenizer(question, padding=True, truncation=True, return_tensors='pt')\n",
    "output_question = model(**tokenized_question)\n",
    "embeddings_question = mean_pooling(output_question[0], tokenized_question['attention_mask'])\n",
    "    \n",
    "# retrieve section/para/sentence\n",
    "section_no = section_retriever(embeddings_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "228fc9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{41: tensor(1.4539, grad_fn=<DotBackward0>),\n",
       " 78: tensor(1.3056, grad_fn=<DotBackward0>),\n",
       " 40: tensor(1.3005, grad_fn=<DotBackward0>),\n",
       " 4: tensor(1.2652, grad_fn=<DotBackward0>),\n",
       " 2: tensor(1.2119, grad_fn=<DotBackward0>)}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b8ff85cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Ripple Counters}\n",
      "\n",
      "A second class of\n",
      "counter drives some of its flip-flops with a clock signal and feeds\n",
      "flip-flop outputs into the clock inputs of its remaining flip-flops,\n",
      "possibly through additional logic.  Such a counter is called a {\n",
      "ripple counter}, because the effect of a clock edge ripples through\n",
      "the flip-flops.  The delay inherent to the ripple effect, along with\n",
      "the complexity of ensuring that timing issues do not render the design\n",
      "unreliable, are the major drawbacks of ripple counters.  Compared with\n",
      "synchronous counters, however, ripple counters consume less energy,\n",
      "and are sometimes used for devices with restricted energy supplies.\n",
      "\n",
      "\n",
      "General ripple counters\n",
      "can be tricky because of timing issues, but certain types are easy.\n",
      "\n",
      "Consider the design of binary ripple counter.  The state diagram for \n",
      "a {3-bit} binary counter is replicated to the right.\n",
      "Looking\n",
      "at the states, notice that the least-significant bit alternates with\n",
      "each state, while higher bits flip whenever the next smaller bit (to\n",
      "the right) transitions from one to zero.  To take advantage of these\n",
      "properties, we use positive edge-triggered D flip-flops with\n",
      "their complemented () outputs wired back to their inputs.\n",
      "The clock input is fed only into the first\n",
      "flip-flop, and the complemented output of each flip-flop is also\n",
      "connected to the clock of the next.\n",
      "\n",
      "\n",
      "{file=part3/figs/lec16-9.eps,width=1.5in}\n",
      "\n",
      "\n",
      "\n",
      "An implementation of a {4-bit} binary ripple counter appears to the\n",
      "right.\n",
      "The order of bits in the figure matches the order used for our synchronous\n",
      "binary counters: least significant on the left, most significant on the\n",
      "right.\n",
      "As you can see from the figure, the technique generalizes to arbitrarily \n",
      "large binary ripple coun-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ters, but the\n",
      "time required for the outputs to settle after a clock edge scales with\n",
      "the number of flip-flops in the counter.  On the other hand, an\n",
      "average of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n",
      "), which reduces the power requirements.{Recall that\n",
      "flip-flops record the clock state internally.  The logical activity\n",
      "required to record such state consumes energy.}\n",
      "\n",
      "\n",
      "\n",
      "Beginning with the state 0000, at the rising clock edge, the left (S_0)\n",
      "flip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\n",
      "falling clock edge and does nothing, leaving the counter in\n",
      "state 0001.  When the next rising clock edge arrives, the left\n",
      "flip-flop toggles back to 0, which the second flip-flop sees as a\n",
      "rising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\n",
      "sees the second flip-flop's change as a falling edge and does nothing,\n",
      "and the state settles as 0010.  We leave verification of the remainder \n",
      "of the cycle as an exercise.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(section_data['41'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f84259a3b330308de677e904dd70af8e4d9960a0b20e6e297421ba1b1b840763"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
