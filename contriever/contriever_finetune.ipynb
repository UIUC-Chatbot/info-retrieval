{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from codecs import EncodedFile\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_scheduler,\n",
    ")\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import json\n",
    "import gzip\n",
    "import random\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:2\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open(\"./textbook_embeddings/fine_tune_cleaned_training_data.json\")\n",
    "data = json.load(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>query_embedding</th>\n",
       "      <th>pos_a</th>\n",
       "      <th>pos_a_embedding</th>\n",
       "      <th>neg_a1</th>\n",
       "      <th>neg_a1_embedding</th>\n",
       "      <th>neg_a2</th>\n",
       "      <th>neg_a2_embedding</th>\n",
       "      <th>neg_a3</th>\n",
       "      <th>neg_a3_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following is not a power of two?A...</td>\n",
       "      <td>[0.0066861999, 0.0055977488, 0.0089797219, 0.0...</td>\n",
       "      <td>102 chapter 3 Digital Logic Structures 3.23 a....</td>\n",
       "      <td>[-0.0184402987, 0.0157946888, 0.0172293726, -0...</td>\n",
       "      <td>626 chapter 19 Dynamic Data Structures in C 1 ...</td>\n",
       "      <td>[-0.0045384252, 0.008056296, -0.0090430574, -0...</td>\n",
       "      <td>19.5 Linked Lists 621 32 { 33 int count = 1; 3...</td>\n",
       "      <td>[0.0085280938, 0.0158668496, -0.0115825301, -0...</td>\n",
       "      <td>19.5 Linked Lists 627 1 int DeleteFlight( char...</td>\n",
       "      <td>[-0.0043733963, 0.0076004323, -0.0046616425, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a digital finite state machine?</td>\n",
       "      <td>[-0.0311171785, -0.0027447108, -0.02551274, -0...</td>\n",
       "      <td>82  3.1.3 Finite State Machines Aﬁnite state m...</td>\n",
       "      <td>[-0.0298135299, 0.002213449, -0.0083825663, -0...</td>\n",
       "      <td>496 chapter 14 Functions 1 #include &lt;stdio.h&gt; ...</td>\n",
       "      <td>[0.0271649808, 0.0061296336, -0.0163586624, -0...</td>\n",
       "      <td>20.2Going from C to C++ 635 1 #include &lt; iostr...</td>\n",
       "      <td>[0.021419961, -0.0074312813, 0.0216170773, -0....</td>\n",
       "      <td>634 chapter 20 Introduction to C++ don’t fret—...</td>\n",
       "      <td>[0.0253242869, 0.0016304518, 0.0185916666, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When is it necessary to use an FSM?</td>\n",
       "      <td>[0.0107197743, 0.0127180228, -0.0080762571, -0...</td>\n",
       "      <td>82  3.1.3 Finite State Machines Aﬁnite state m...</td>\n",
       "      <td>[-0.0298135299, 0.002213449, -0.0083825663, -0...</td>\n",
       "      <td>16.3 Arrays 555 1 #include &lt;stdio.h&gt; 2 #define...</td>\n",
       "      <td>[0.0145583516, -0.000528549, -0.0052345362, 0....</td>\n",
       "      <td>558 chapter 16 Pointersand Arrays 1 #include &lt;...</td>\n",
       "      <td>[-0.0090973862, 0.0067505054, -0.0064539216, -...</td>\n",
       "      <td>750 appendix D The C ProgrammingLanguage D.9.2...</td>\n",
       "      <td>[0.0143375332, -0.0013394544, -0.0120525314, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the best design in terms of other metr...</td>\n",
       "      <td>[0.0030658261, 0.0098778214, -0.0074649183, -0...</td>\n",
       "      <td>34  In 1952, Edward Veitch wrote an article on...</td>\n",
       "      <td>[0.0106401099, 0.0134047549, 0.0100638559, -0....</td>\n",
       "      <td>18.5 I/O from Files 603 to specify the stream ...</td>\n",
       "      <td>[0.0011227614, -0.0046986854, -0.0019491339, -...</td>\n",
       "      <td>334 chapter 9 I/O 01 ; Service Routine for Key...</td>\n",
       "      <td>[-0.0122134993, -0.0073032021, -0.0129465852, ...</td>\n",
       "      <td>674 appendix A The LC-3 ISA Unused OpcodeAssem...</td>\n",
       "      <td>[-0.0120490398, -0.0037330692, -0.0215933118, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the design process for a counter with ...</td>\n",
       "      <td>[-0.0169024002, 0.0157117043, 0.00269104, 0.00...</td>\n",
       "      <td>94  A counter with a single counting state, of...</td>\n",
       "      <td>[-0.0176479928, -0.0052896538, 0.0076379757, -...</td>\n",
       "      <td>582 chapter 17 Recursion 1// Thisfunction retu...</td>\n",
       "      <td>[0.0016060983, 0.017569419, -0.0037719097, -0....</td>\n",
       "      <td>19.5 Linked Lists 627 1 int DeleteFlight( char...</td>\n",
       "      <td>[-0.0043733963, 0.0076004323, -0.0046616425, -...</td>\n",
       "      <td>674 appendix A The LC-3 ISA Unused OpcodeAssem...</td>\n",
       "      <td>[-0.0120490398, -0.0037330692, -0.0215933118, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  Which of the following is not a power of two?A...   \n",
       "1            What is a digital finite state machine?   \n",
       "2                When is it necessary to use an FSM?   \n",
       "3  What is the best design in terms of other metr...   \n",
       "4  What is the design process for a counter with ...   \n",
       "\n",
       "                                     query_embedding  \\\n",
       "0  [0.0066861999, 0.0055977488, 0.0089797219, 0.0...   \n",
       "1  [-0.0311171785, -0.0027447108, -0.02551274, -0...   \n",
       "2  [0.0107197743, 0.0127180228, -0.0080762571, -0...   \n",
       "3  [0.0030658261, 0.0098778214, -0.0074649183, -0...   \n",
       "4  [-0.0169024002, 0.0157117043, 0.00269104, 0.00...   \n",
       "\n",
       "                                               pos_a  \\\n",
       "0  102 chapter 3 Digital Logic Structures 3.23 a....   \n",
       "1  82  3.1.3 Finite State Machines Aﬁnite state m...   \n",
       "2  82  3.1.3 Finite State Machines Aﬁnite state m...   \n",
       "3  34  In 1952, Edward Veitch wrote an article on...   \n",
       "4  94  A counter with a single counting state, of...   \n",
       "\n",
       "                                     pos_a_embedding  \\\n",
       "0  [-0.0184402987, 0.0157946888, 0.0172293726, -0...   \n",
       "1  [-0.0298135299, 0.002213449, -0.0083825663, -0...   \n",
       "2  [-0.0298135299, 0.002213449, -0.0083825663, -0...   \n",
       "3  [0.0106401099, 0.0134047549, 0.0100638559, -0....   \n",
       "4  [-0.0176479928, -0.0052896538, 0.0076379757, -...   \n",
       "\n",
       "                                              neg_a1  \\\n",
       "0  626 chapter 19 Dynamic Data Structures in C 1 ...   \n",
       "1  496 chapter 14 Functions 1 #include <stdio.h> ...   \n",
       "2  16.3 Arrays 555 1 #include <stdio.h> 2 #define...   \n",
       "3  18.5 I/O from Files 603 to specify the stream ...   \n",
       "4  582 chapter 17 Recursion 1// Thisfunction retu...   \n",
       "\n",
       "                                    neg_a1_embedding  \\\n",
       "0  [-0.0045384252, 0.008056296, -0.0090430574, -0...   \n",
       "1  [0.0271649808, 0.0061296336, -0.0163586624, -0...   \n",
       "2  [0.0145583516, -0.000528549, -0.0052345362, 0....   \n",
       "3  [0.0011227614, -0.0046986854, -0.0019491339, -...   \n",
       "4  [0.0016060983, 0.017569419, -0.0037719097, -0....   \n",
       "\n",
       "                                              neg_a2  \\\n",
       "0  19.5 Linked Lists 621 32 { 33 int count = 1; 3...   \n",
       "1  20.2Going from C to C++ 635 1 #include < iostr...   \n",
       "2  558 chapter 16 Pointersand Arrays 1 #include <...   \n",
       "3  334 chapter 9 I/O 01 ; Service Routine for Key...   \n",
       "4  19.5 Linked Lists 627 1 int DeleteFlight( char...   \n",
       "\n",
       "                                    neg_a2_embedding  \\\n",
       "0  [0.0085280938, 0.0158668496, -0.0115825301, -0...   \n",
       "1  [0.021419961, -0.0074312813, 0.0216170773, -0....   \n",
       "2  [-0.0090973862, 0.0067505054, -0.0064539216, -...   \n",
       "3  [-0.0122134993, -0.0073032021, -0.0129465852, ...   \n",
       "4  [-0.0043733963, 0.0076004323, -0.0046616425, -...   \n",
       "\n",
       "                                              neg_a3  \\\n",
       "0  19.5 Linked Lists 627 1 int DeleteFlight( char...   \n",
       "1  634 chapter 20 Introduction to C++ don’t fret—...   \n",
       "2  750 appendix D The C ProgrammingLanguage D.9.2...   \n",
       "3  674 appendix A The LC-3 ISA Unused OpcodeAssem...   \n",
       "4  674 appendix A The LC-3 ISA Unused OpcodeAssem...   \n",
       "\n",
       "                                    neg_a3_embedding  \n",
       "0  [-0.0043733963, 0.0076004323, -0.0046616425, -...  \n",
       "1  [0.0253242869, 0.0016304518, 0.0185916666, -0....  \n",
       "2  [0.0143375332, -0.0013394544, -0.0120525314, -...  \n",
       "3  [-0.0120490398, -0.0037330692, -0.0215933118, ...  \n",
       "4  [-0.0120490398, -0.0037330692, -0.0215933118, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to list of lists\n",
    "triplets = []\n",
    "for row in df.iterrows():\n",
    "    tmp = []\n",
    "    tmp.append(row[1]['query'])\n",
    "    tmp.append(row[1]['pos_a'])\n",
    "    tmp2 = []\n",
    "    tmp2.append(row[1]['neg_a1'])\n",
    "    tmp2.append(row[1]['neg_a2'])\n",
    "    tmp2.append(row[1]['neg_a3'])\n",
    "    tmp.append(tmp2)\n",
    "    triplets.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['626 chapter 19 Dynamic Data Structures in C 1 int AddFlight(Flight * newPlane, Flight **list) 2{ 3 Flight *previous = NULL; 4 Flight *current = *list; 5 int IDcompare; 6 7 while (current != NULL) { 8 IDcompare = strcmp (newPlane->ID, current->ID); 9 // returns 0 if equal 10 // < 0 if newPlane->ID is less than current->ID 11 // > 0 if newPlane->ID is greater than current->ID 12 13 if (IDcompare = = 0) 14 return -1; // Exists! 15 } 16 else if ( IDcompare < 0) { 17 // Add newPlane in between previous and current nodes 18 newPlane->next = current; 19 if (previous == NULL) 20 *list = newPlane; // Add At Head 21 else 22 previous->next = newPlane; // Add to Middle 23 return 0; 24 } 25 else { 26 // Continue traversing thru the list 27 previous = current; 28 current = current->next; 29 } 30 } 31 newPlane->next = NULL; 32 if (previous == NULL) 33 *list = newPlane; // Empty List 34 else 35 previous->next = newPlane; // Add At Tail 36 return 0; 37 } Figure 19.8 Source code for complete AddFlight function. order to assign a new head node after the delete operation. Figure 19.9 provides the complete source code for DeleteFlight . 19.5.4 Arrays vs. Linked Lists We can now replace our core array data structure in our ﬂight tracker app with the more dynamic linked list. Throughout this chapter, we’ve discussed some of the advantages in doing so. But as with many things in computing, there are tradeoﬀs involved: linked lists provide some advantages over arrays, but they also have some disadvantages. Wisely choosing which method of data organization to', '19.5 Linked Lists 621 32 { 33 int count = 1; 34 35 printf(\"Aircraft in Airspace --------------------------\\\\n\"); 36 while (list != NULL) { 37 printf(\"Aircraft : %d\\\\n\",count); 38 printf(\"ID       : %s\\\\n\",list->ID); 39 printf(\"Altitude : %d\\\\n\",list->altitude); 40 printf(\"Longitude: %d\\\\n\",list->longitude); 41 printf(\"Heading  : %d\\\\n\",list->heading); 42 printf(\"Airspeed : %f\\\\n\",list->airSpeed); 43 printf(\" -----------------------------\\\\n\"); 44 count = count + 1; 45 list = list->next; 46 } 47 printf (\"\\\\n\\\\n\"); 48 } 49 50 int main(void) 51 { 52 Flight *airspace = NULL; 53 Flight *newPlane = NULL; 54 55 newPlane = CreateFlight (\"ZA123\", 1000, 3233, 56 2516, 392, 3493.20); 57 if (AddFlight (newPlane, & airspace) == 0) 58 printf(\" Successful add of flight %s\\\\n\",newPlane->ID); 59 . 60 . 61 . 62 if (DeleteFlight (\"ZA123\", &airspace) == 0) 63 printf(\" Successful removal of  flight %s\\\\n\", \"ZZ\"); 64 . 65 . 66 . 67 } Figure 19.5 The support functions for our airspace tracker (continued Fig. 19.5 from previous page.) for the support functions for our linked list–based ﬂight tracker. The structure def- inition is just as we developed previously, with the addition of the Flight *next pointer to enable us to link nodes together. The function CreateFlight takes as arguments the various properties of an aircraft to create a new node via malloc . The function then returns a pointer to this new node. The function PrintAirspace prints all the aircraft in the airspace by travers- ing the linked list, which is provided as an input parameter. Notice that the while loop is the analog of the forloop we would typically use for traversing a ﬁxed- size array. Since we don’t know the number of nodes in the list, we use a while loop to keep traversing from node to node until we reach the NULL pointer. And', \"19.5 Linked Lists 627 1 int DeleteFlight( char *planeID, Flight **list) 2{ 3 Flight *previous = NULL; 4 Flight *current = *list; 5 int IDcompare; 6 7 while (current != NULL) { 8 IDcompare = strcmp( planeID, current->ID); 9// returns 0 if equal 10 // < 0 if Plane->ID is less than current->ID 11 // > 0 if Plane->ID is greater than current->ID 12 if (IDcompare == 0) { 13 // Found node to remove! 14 if (previous == NULL) 15 *list = current->next; // Del At Head 16 else 17 previous->next = current->next; // Del from Mid/Tail 18 free(current); 19 return 0; 20 } 21 else if ( IDcompare < 0) 22 return -1; // Doesn't Exist 23 else { 24 // Continue traversing thru the list 25 previous = current; 26 current = current->next; 27 } 28 } 29// Traversed the whole list. Doesn't Exist 30 return -1; 31 } Figure 19.9 Source code for complete DeleteFlight function. use in a particular situation requires consideration of these tradeoﬀs (and also a deeper sense of how these structures are implemented at the lower level). Let’s ﬁrst examine the impact to memory space. Arrays are quite memory eﬃcient. If we create an array of 1000 integers, we will be allocated 1000 inte- gers’ worth of memory space. Additional storage is not required. If the array is dynamically allocated on the heap, there is likely some additional overhead to keep track of the block of memory, but that is small in relation to the actual array. Linked lists, in contrast, require a pointer per node to link to the next node. Also, since the nodes are individually allocated on the heap, each node will incur addi- tional dynamic allocation overhead. If the node size is small, then this overhead can be a signiﬁcant fraction of overall data structure size. While arrays are eﬃ- cient in terms of allocation, they suﬀer in our inability to precisely size them to our needs. We often need to declare strings that are long enough to hold the longest string we expect to encounter, which is larger than necessary for the typical case.\"]\n",
      "[[('Which of the following is not a power of two?A) 4B) 8 C) 16 D) 24', '102 chapter 3 Digital Logic Structures 3.23 a.Given four inputs A,B,C, and Dand one output Z, create a truth table for a circuit with at least seven input combinations generating 1s at the output. (How many rows will this truth table have?) b.Now that you have a truth table, generate the gate-level logic circuit that implements this truth table. Use the implementation algorithm referred to in Section 3.3.4. 3.24 Implement the following functions using AND, OR, and NOT logic gates. The inputs are A,B, and the output is F. a. F has the value 1 only if Ahas the value 0 and Bhas the value 1. b. F has the value 1 only if Ahas the value 1 and Bhas the value 0. c.Use your answers from parts aand bto implement a one-bit adder. The truth table for the one-bit adder is given below. AB Sum 00 0 01 1 10 1 11 0 d.Is it possible to create a four-bit adder (a circuit that will correctly add two 4-bit quantities) using only four copies of the logic diagram from part c? If not, what information is missing? Hint: When A=1 and B=1, a sum of 0 is produced. What information is lost? 3.25 Logic circuit 1 in Figure 3.39 has inputs A,B,C. Logic circuit 2 in Figure 3.40 has inputs Aand B. Both logic circuits have an output D. There is a fundamental diﬀerence between the behavioral characteristics of these two circuits. What is it? Hint: What happens when the voltage at input Agoes from 0 to 1 in both circuits? BDC A Figure 3.39 Logic circuit 1 for Exercise 3.25.A D B Figure 3.40 Logic circuit 2 for Exercise 3.25.')], [('Which of the following is not a power of two?A) 4B) 8 C) 16 D) 24', '626 chapter 19 Dynamic Data Structures in C 1 int AddFlight(Flight * newPlane, Flight **list) 2{ 3 Flight *previous = NULL; 4 Flight *current = *list; 5 int IDcompare; 6 7 while (current != NULL) { 8 IDcompare = strcmp (newPlane->ID, current->ID); 9 // returns 0 if equal 10 // < 0 if newPlane->ID is less than current->ID 11 // > 0 if newPlane->ID is greater than current->ID 12 13 if (IDcompare = = 0) 14 return -1; // Exists! 15 } 16 else if ( IDcompare < 0) { 17 // Add newPlane in between previous and current nodes 18 newPlane->next = current; 19 if (previous == NULL) 20 *list = newPlane; // Add At Head 21 else 22 previous->next = newPlane; // Add to Middle 23 return 0; 24 } 25 else { 26 // Continue traversing thru the list 27 previous = current; 28 current = current->next; 29 } 30 } 31 newPlane->next = NULL; 32 if (previous == NULL) 33 *list = newPlane; // Empty List 34 else 35 previous->next = newPlane; // Add At Tail 36 return 0; 37 } Figure 19.8 Source code for complete AddFlight function. order to assign a new head node after the delete operation. Figure 19.9 provides the complete source code for DeleteFlight . 19.5.4 Arrays vs. Linked Lists We can now replace our core array data structure in our ﬂight tracker app with the more dynamic linked list. Throughout this chapter, we’ve discussed some of the advantages in doing so. But as with many things in computing, there are tradeoﬀs involved: linked lists provide some advantages over arrays, but they also have some disadvantages. Wisely choosing which method of data organization to')], [('Which of the following is not a power of two?A) 4B) 8 C) 16 D) 24', '19.5 Linked Lists 621 32 { 33 int count = 1; 34 35 printf(\"Aircraft in Airspace --------------------------\\\\n\"); 36 while (list != NULL) { 37 printf(\"Aircraft : %d\\\\n\",count); 38 printf(\"ID       : %s\\\\n\",list->ID); 39 printf(\"Altitude : %d\\\\n\",list->altitude); 40 printf(\"Longitude: %d\\\\n\",list->longitude); 41 printf(\"Heading  : %d\\\\n\",list->heading); 42 printf(\"Airspeed : %f\\\\n\",list->airSpeed); 43 printf(\" -----------------------------\\\\n\"); 44 count = count + 1; 45 list = list->next; 46 } 47 printf (\"\\\\n\\\\n\"); 48 } 49 50 int main(void) 51 { 52 Flight *airspace = NULL; 53 Flight *newPlane = NULL; 54 55 newPlane = CreateFlight (\"ZA123\", 1000, 3233, 56 2516, 392, 3493.20); 57 if (AddFlight (newPlane, & airspace) == 0) 58 printf(\" Successful add of flight %s\\\\n\",newPlane->ID); 59 . 60 . 61 . 62 if (DeleteFlight (\"ZA123\", &airspace) == 0) 63 printf(\" Successful removal of  flight %s\\\\n\", \"ZZ\"); 64 . 65 . 66 . 67 } Figure 19.5 The support functions for our airspace tracker (continued Fig. 19.5 from previous page.) for the support functions for our linked list–based ﬂight tracker. The structure def- inition is just as we developed previously, with the addition of the Flight *next pointer to enable us to link nodes together. The function CreateFlight takes as arguments the various properties of an aircraft to create a new node via malloc . The function then returns a pointer to this new node. The function PrintAirspace prints all the aircraft in the airspace by travers- ing the linked list, which is provided as an input parameter. Notice that the while loop is the analog of the forloop we would typically use for traversing a ﬁxed- size array. Since we don’t know the number of nodes in the list, we use a while loop to keep traversing from node to node until we reach the NULL pointer. And')], [('Which of the following is not a power of two?A) 4B) 8 C) 16 D) 24', \"19.5 Linked Lists 627 1 int DeleteFlight( char *planeID, Flight **list) 2{ 3 Flight *previous = NULL; 4 Flight *current = *list; 5 int IDcompare; 6 7 while (current != NULL) { 8 IDcompare = strcmp( planeID, current->ID); 9// returns 0 if equal 10 // < 0 if Plane->ID is less than current->ID 11 // > 0 if Plane->ID is greater than current->ID 12 if (IDcompare == 0) { 13 // Found node to remove! 14 if (previous == NULL) 15 *list = current->next; // Del At Head 16 else 17 previous->next = current->next; // Del from Mid/Tail 18 free(current); 19 return 0; 20 } 21 else if ( IDcompare < 0) 22 return -1; // Doesn't Exist 23 else { 24 // Continue traversing thru the list 25 previous = current; 26 current = current->next; 27 } 28 } 29// Traversed the whole list. Doesn't Exist 30 return -1; 31 } Figure 19.9 Source code for complete DeleteFlight function. use in a particular situation requires consideration of these tradeoﬀs (and also a deeper sense of how these structures are implemented at the lower level). Let’s ﬁrst examine the impact to memory space. Arrays are quite memory eﬃcient. If we create an array of 1000 integers, we will be allocated 1000 inte- gers’ worth of memory space. Additional storage is not required. If the array is dynamically allocated on the heap, there is likely some additional overhead to keep track of the block of memory, but that is small in relation to the actual array. Linked lists, in contrast, require a pointer per node to link to the next node. Also, since the nodes are individually allocated on the heap, each node will incur addi- tional dynamic allocation overhead. If the node size is small, then this overhead can be a signiﬁcant fraction of overall data structure size. While arrays are eﬃ- cient in terms of allocation, they suﬀer in our inability to precisely size them to our needs. We often need to declare strings that are long enough to hold the longest string we expect to encounter, which is larger than necessary for the typical case.\")]]\n"
     ]
    }
   ],
   "source": [
    "query_doc_pairs = [[] for _ in  range(1+3)]\n",
    "for row in triplets:\n",
    "    # TODO @josh\n",
    "    # row[0] = query\n",
    "    # row[1] = pos\n",
    "    # row[2] = neg\n",
    "    query_text = row[0]\n",
    "    # pos\n",
    "    query_doc_pairs[0].append((query_text, row[1]))\n",
    "    # negs\n",
    "    print(row[2])\n",
    "    for neg_id, neg in enumerate(row[2]):\n",
    "        query_doc_pairs[1+neg_id].append((query_text, neg))\n",
    "    print(query_doc_pairs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0300, 0.0100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(0.03)\n",
    "b = torch.tensor(0.01)\n",
    "torch.stack([a,b], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSMARCOData(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        triplets_path: str,\n",
    "        langs,\n",
    "        max_seq_length: int = 250,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        num_negs: int = 3,\n",
    "        cross_lingual_chance: float = 0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.triplets_path = triplets_path\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.langs = langs\n",
    "        self.num_negs = num_negs\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.cross_lingual_chance = cross_lingual_chance  #Probability for cross-lingual batches\n",
    "\n",
    "        #def setup(self, stage: str):\n",
    "        print(f\"!!!!!!!!!!!!!!!!!! SETUP {os.getpid()}  !!!!!!!!!!!!!!!\")\n",
    "\n",
    "        #Get the queries\n",
    "        self.queries = {lang: {} for lang in self.langs}\n",
    "    \n",
    "        for lang in self.langs:\n",
    "            for row in tqdm.tqdm(load_dataset('unicamp-dl/mmarco', f'queries-{lang}')['train'], desc=lang):\n",
    "                self.queries[lang][row['id']] = row['text']\n",
    "\n",
    "        #Get the passages\n",
    "        self.collections = {lang: load_dataset('unicamp-dl/mmarco', f'collection-{lang}')['collection'] for lang in self.langs}\n",
    "\n",
    "        #Get the triplets\n",
    "        # with gzip.open(self.triplets_path, 'rt') as fIn:\n",
    "        #     self.triplets = [json.loads(line) for line in tqdm.tqdm(fIn, desc=\"triplets\", total=502938)] \n",
    "        #     \"\"\"\n",
    "        #     self.triplets = []\n",
    "        #     for line in tqdm.tqdm(fIn):\n",
    "        #         self.triplets.append(json.loads(line))\n",
    "        #         if len(self.triplets) >= 1000:\n",
    "        #             break\n",
    "        #     \"\"\"\n",
    "\n",
    "        # asmita's paths\n",
    "        s = open(\"textbook_embeddings/fine_tune_cleaned_training_data.json\")\n",
    "        data = json.load(s)\n",
    "        df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "        self.triplets = df[['query', 'pos_a', 'neg_a1', 'neg_a2', 'neg_a3']]\n",
    "\n",
    "        # self.bad_data = []\n",
    "        # for dataset in [third, fourth]:\n",
    "        #     for row in dataset:\n",
    "        #         self.bad_data.append(row['text'])\n",
    "\n",
    "        # # create tirplets+ of <question, good answer (pos), and 3 bad answers (neg1, neg2, neg3)>\n",
    "        # self.triplets = []\n",
    "        # for dataset in [first, second]:\n",
    "        #     for row in dataset:\n",
    "        #         itr_counter = 0 \n",
    "        #         # Ensure our negative samples are not the same as each other (and that neg not == pos sample)\n",
    "                \n",
    "        #         neg1, neg2, neg3, = random.choice(self.bad_data), random.choice(self.bad_data), random.choice(self.bad_data)\n",
    "        #         while ( neg1 == neg2 or neg1 == neg3 or neg2 == neg3 ) and ( any(neg_ex in row['GPT-3-Semantic-Search-Generations']['answer'] for neg_ex in [neg1, neg2, neg3]) ) and itr_counter < 50:\n",
    "        #             neg1, neg2, neg3, = random.choice(self.bad_data), random.choice(self.bad_data), random.choice(self.bad_data)\n",
    "        #             itr_counter += 1\n",
    "        #         if itr_counter == 50:\n",
    "        #             print(\"❌❌❌ WARNING: 50 iterations reached, negs may be equal ❌❌❌\")\n",
    "        #         self.triplets.append([row['GPT-3-Semantic-Search-Generations']['question'], row['GPT-3-Semantic-Search-Generations']['answer'],[neg1, neg2, neg3]])\n",
    "\n",
    "            \n",
    "        def collate_fn(self, batch):\n",
    "            '''\n",
    "            # EXPECED DATA FORMAT BEFORE TOKENIZATION\n",
    "            query_doc_pairs_OUR_INTERPRETATION = [\n",
    "                [('query1', 'pos1'), ('query2', 'po2')],\n",
    "                [('query1', 'neg1'), ('query2', 'neg2')],\n",
    "                [],\n",
    "                [],\n",
    "                []\n",
    "            ]\n",
    "            '''\n",
    "            #Create data for list-rank-loss\n",
    "            query_doc_pairs = [[] for _ in  range(1+3)]\n",
    "                \n",
    "            #example_train_data = [['query', 'pos', 'neg'],['query2', 'po2', 'neg2']]\n",
    "\n",
    "            # create a list of lists\n",
    "            query_doc_pairs = []\n",
    "            for row in batch.iterrows():\n",
    "                tmp_row = []\n",
    "                tmp_row.append(row[1]['query'])\n",
    "                tmp_row.append(row[1]['pos_a'])\n",
    "                tmp_row.append(row[1]['neg_a1'])\n",
    "                tmp_row.append(row[1]['neg_a2'])\n",
    "                tmp_row.append(row[1]['neg_a3'])\n",
    "\n",
    "                query_doc_pairs.append(tmp_row)\n",
    "\n",
    "                ''' \n",
    "                future refernece for multiple negs\n",
    "                # for num_neg, neg_id in enumerate(neg_ids):\n",
    "                    # query_doc_pairs[1+num_neg].append((query_text, row[2]))\n",
    "                '''\n",
    "\n",
    "            ''' ORIGINAL CODE\n",
    "            query_doc_pairs = [[] for _ in  range(1+self.num_negs)]\n",
    "            cross_lingual_batch = random.random() < self.cross_lingual_chance \n",
    "            for row in batch:\n",
    "                qid = row['qid']\n",
    "                print('qid', qid)\n",
    "                pos_id = random.choice(row['pos'])\n",
    "                query_lang = random.choice(self.langs)\n",
    "                query_text = self.queries[query_lang][qid]\n",
    "                    \n",
    "                doc_lang = random.choice(self.langs) if cross_lingual_batch else query_lang \n",
    "                query_doc_pairs[0].append((query_text, self.collections[doc_lang][pos_id]['text']))\n",
    "                dense_bm25_neg = list(set(row['dense_neg'] + row['bm25_neg']))\n",
    "                neg_ids = random.sample(dense_bm25_neg, self.num_negs)\n",
    "                for num_neg, neg_id in enumerate(neg_ids):\n",
    "                    doc_lang = random.choice(self.langs) if cross_lingual_batch else query_lang\n",
    "                    query_doc_pairs[1+num_neg].append((query_text, self.collections[doc_lang][neg_id]['text']))\n",
    "            '''\n",
    "            print(\"query_doc_pairs\", query_doc_pairs)\n",
    "        \n",
    "            #Now tokenize the data\n",
    "            features = [self.tokenizer(qd_pair, max_length=self.max_seq_length, padding=True, truncation='only_second', return_tensors=\"pt\") for qd_pair in query_doc_pairs]\n",
    "            \n",
    "            return features\n",
    "\n",
    "        def train_dataloader(self):\n",
    "            return DataLoader(self.triplets, shuffle=True, batch_size=self.train_batch_size, num_workers=1, pin_memory=True, collate_fn=self.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!! SETUP 28664  !!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "BuilderConfig queries-a not found. Available: ['arabic', 'chinese', 'dutch', 'english', 'french', 'german', 'hindi', 'indonesian', 'italian', 'japanese', 'portuguese', 'russian', 'spanish', 'vietnamese', 'collection-arabic', 'collection-chinese', 'collection-dutch', 'collection-english', 'collection-french', 'collection-german', 'collection-hindi', 'collection-indonesian', 'collection-italian', 'collection-japanese', 'collection-portuguese', 'collection-russian', 'collection-spanish', 'collection-vietnamese', 'queries-arabic', 'queries-chinese', 'queries-dutch', 'queries-english', 'queries-french', 'queries-german', 'queries-hindi', 'queries-indonesian', 'queries-italian', 'queries-japanese', 'queries-portuguese', 'queries-russian', 'queries-spanish', 'queries-vietnamese', 'runs-arabic', 'runs-chinese', 'runs-dutch', 'runs-english', 'runs-french', 'runs-german', 'runs-hindi', 'runs-indonesian', 'runs-italian', 'runs-japanese', 'runs-portuguese', 'runs-russian', 'runs-spanish', 'runs-vietnamese']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dm \u001b[39m=\u001b[39m MSMARCOData(model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfacebook/contriever-msmarco\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      2\u001b[0m                   langs\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39margs.langs\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m                   train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m                   triplets_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./msmarco-triplets.jsonl.gz\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m                   cross_lingual_chance\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m3\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                   num_negs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m dm:\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mMSMARCOData.__init__\u001b[0;34m(self, model_name, triplets_path, langs, max_seq_length, train_batch_size, eval_batch_size, num_negs, cross_lingual_chance, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries \u001b[39m=\u001b[39m {lang: {} \u001b[39mfor\u001b[39;00m lang \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlangs}\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m lang \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlangs:\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(load_dataset(\u001b[39m'\u001b[39;49m\u001b[39municamp-dl/mmarco\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mqueries-\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m], desc\u001b[39m=\u001b[39mlang):\n\u001b[1;32m     33\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries[lang][row[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[39m#Get the passages\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/datasets/load.py:1735\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[1;32m   1734\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1735\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1736\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1737\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1738\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1739\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1740\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1741\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1742\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1743\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1744\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1745\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1746\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1747\u001b[0m )\n\u001b[1;32m   1749\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/datasets/load.py:1519\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m   1518\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1519\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[1;32m   1520\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1521\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m   1522\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1523\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1524\u001b[0m     \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[1;32m   1525\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1526\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1527\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs,\n\u001b[1;32m   1528\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[1;32m   1531\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1357\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[0;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, writer_batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1357\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1358\u001b[0m     \u001b[39m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m     \u001b[39m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m     \u001b[39m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m     \u001b[39m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer_batch_size \u001b[39m=\u001b[39m writer_batch_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/datasets/builder.py:322\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     config_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m data_dir\n\u001b[0;32m--> 322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_builder_config(\n\u001b[1;32m    323\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[1;32m    324\u001b[0m     custom_features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    325\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[39m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[39m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/lib/python3.10/site-packages/datasets/builder.py:462\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     builder_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder_configs\u001b[39m.\u001b[39mget(config_name)\n\u001b[1;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m builder_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBUILDER_CONFIGS:\n\u001b[0;32m--> 462\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBuilderConfig \u001b[39m\u001b[39m{\u001b[39;00mconfig_name\u001b[39m}\u001b[39;00m\u001b[39m not found. Available: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder_configs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    466\u001b[0m \u001b[39m# if not using an existing config, then create a new config on the fly\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m builder_config:\n",
      "\u001b[0;31mValueError\u001b[0m: BuilderConfig queries-a not found. Available: ['arabic', 'chinese', 'dutch', 'english', 'french', 'german', 'hindi', 'indonesian', 'italian', 'japanese', 'portuguese', 'russian', 'spanish', 'vietnamese', 'collection-arabic', 'collection-chinese', 'collection-dutch', 'collection-english', 'collection-french', 'collection-german', 'collection-hindi', 'collection-indonesian', 'collection-italian', 'collection-japanese', 'collection-portuguese', 'collection-russian', 'collection-spanish', 'collection-vietnamese', 'queries-arabic', 'queries-chinese', 'queries-dutch', 'queries-english', 'queries-french', 'queries-german', 'queries-hindi', 'queries-indonesian', 'queries-italian', 'queries-japanese', 'queries-portuguese', 'queries-russian', 'queries-spanish', 'queries-vietnamese', 'runs-arabic', 'runs-chinese', 'runs-dutch', 'runs-english', 'runs-french', 'runs-german', 'runs-hindi', 'runs-indonesian', 'runs-italian', 'runs-japanese', 'runs-portuguese', 'runs-russian', 'runs-spanish', 'runs-vietnamese']"
     ]
    }
   ],
   "source": [
    "dm = MSMARCOData(model_name='facebook/contriever-msmarco',\n",
    "                  langs='args.langs',\n",
    "                  train_batch_size=1,\n",
    "                  triplets_path='./msmarco-triplets.jsonl.gz',\n",
    "                  cross_lingual_chance='3',\n",
    "                  num_negs=3)\n",
    "for i in dm:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListRankLoss(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        learning_rate: float = 2e-5,\n",
    "        warmup_steps: int = 1000,\n",
    "        weight_decay: float = 0.01,\n",
    "        train_batch_size: int = 32,\n",
    "        eval_batch_size: int = 32,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        print(self.hparams)\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(model_name, num_labels=1)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=self.config)\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        self.global_train_step = 0\n",
    "        \n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pred_scores = []\n",
    "        print(\"batch\", batch)\n",
    "        print(\"batch a 0\", batch[0])\n",
    "        scores = torch.tensor([0] * len(batch[0]['input_ids']), device=self.model.device)\n",
    "   \n",
    "        for feature in batch:\n",
    "            pred_scores.append(self(**feature).logits.squeeze())\n",
    "\n",
    "        pred_scores = torch.stack(pred_scores, 1)\n",
    "        loss_value = self.loss_fct(pred_scores, scores)\n",
    "        self.global_train_step += 1\n",
    "        self.log('global_train_step', self.global_train_step)\n",
    "        self.log(\"train/loss\", loss_value)\n",
    "\n",
    "        return loss_value\n",
    "     \n",
    "\n",
    "    def setup(self, stage=None) -> None:\n",
    "        if stage != \"fit\":\n",
    "            return\n",
    "        # Get dataloader by calling it - train_dataloader() is called after setup() by default\n",
    "        train_loader = self.trainer.datamodule.train_dataloader()\n",
    "\n",
    "        # Calculate total steps\n",
    "        tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)\n",
    "        ab_size = self.trainer.accumulate_grad_batches\n",
    "        self.total_steps = (len(train_loader) // ab_size) * self.trainer.max_epochs\n",
    "\n",
    "        print(\"{tb_size=}\")\n",
    "        print(\"{ab_size=}\")\n",
    "        print(\"{len(train_loader)=}\")\n",
    "        print(\"{self.total_steps=}\")\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer =  torch.optim.AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate)\n",
    "\n",
    "        lr_scheduler = get_scheduler(\n",
    "            name=\"linear\",\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.total_steps,\n",
    "        )\n",
    "\n",
    "        scheduler = {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    dm = MSMARCOData(\n",
    "        model_name=args.model,\n",
    "        langs=args.langs,\n",
    "        # triplets_path='./msmarco-hard-triplets.jsonl.gz',\n",
    "        triplets_path='./msmarco-triplets.jsonl.gz',\n",
    "        train_batch_size=args.batch_size,\n",
    "        cross_lingual_chance=args.cross_lingual_chance,\n",
    "        num_negs=args.num_negs\n",
    "    )\n",
    "    output_dir = f\"output/{args.model.replace('/', '-')}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    print(\"Output_dir:\", output_dir)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"multilingual-cross-encoder\", name=output_dir.split(\"/\")[-1])\n",
    "\n",
    "    train_script_path = os.path.join(output_dir, 'train_script.py')\n",
    "    copyfile(__file__, train_script_path)\n",
    "    with open(train_script_path, 'a') as fOut:\n",
    "        fOut.write(\"\\n\\n# Script was called via:\\n#python \" + \" \".join(sys.argv))\n",
    "\n",
    "    \n",
    "    # saves top-K checkpoints based on \"val_loss\" metric\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        every_n_train_steps=25000,\n",
    "        save_top_k=5,\n",
    "        monitor=\"global_train_step\",\n",
    "        mode=\"max\",\n",
    "        dirpath=output_dir,\n",
    "        filename=\"ckpt-{global_train_step}\",\n",
    "    )\n",
    "\n",
    "\n",
    "    model = ListRankLoss(model_name=args.model)\n",
    "\n",
    "    trainer = Trainer(max_epochs=args.epochs, \n",
    "                      accelerator=\"gpu\", \n",
    "                      devices=args.num_gpus, \n",
    "                      precision=args.precision, \n",
    "                      strategy=args.strategy,    \n",
    "                      default_root_dir=output_dir,\n",
    "                      callbacks=[checkpoint_callback],\n",
    "                      logger=wandb_logger\n",
    "                      )\n",
    "\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "\n",
    "    #Save final HF model \n",
    "    final_path = os.path.join(output_dir, \"final\")\n",
    "    dm.tokenizer.save_pretrained(final_path)\n",
    "    model.model.save_pretrained(final_path)\n",
    "\n",
    "  \n",
    "def eval(args):\n",
    "    import ir_datasets\n",
    "    \n",
    " \n",
    "    model = ListRankLoss.load_from_checkpoint(args.ckpt)\n",
    "    hf_model = model.model.cuda()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.hparams.model_name)\n",
    "\n",
    "    dev_qids = set()\n",
    "\n",
    "    dev_queries = {}\n",
    "    dev_rel_docs = {}\n",
    "    needed_pids = set()\n",
    "    needed_qids = set()\n",
    "\n",
    "    corpus = {}\n",
    "    retrieved_docs = {}\n",
    "\n",
    "    dataset = ir_datasets.load(\"msmarco-passage/dev/small\")\n",
    "    for query in dataset.queries_iter():\n",
    "        dev_qids.add(query.query_id)\n",
    "\n",
    "    \n",
    "    with open('data/qrels.dev.tsv') as fIn:\n",
    "        for line in fIn:\n",
    "            qid, _, pid, _ = line.strip().split('\\t')\n",
    "\n",
    "            if qid not in dev_qids:\n",
    "                continue\n",
    "\n",
    "            if qid not in dev_rel_docs:\n",
    "                dev_rel_docs[qid] = set()\n",
    "            dev_rel_docs[qid].add(pid)\n",
    "\n",
    "            retrieved_docs[qid] = set()\n",
    "            needed_qids.add(qid)\n",
    "            needed_pids.add(pid)\n",
    "\n",
    "    for query in dataset.queries_iter():\n",
    "        qid = query.query_id\n",
    "        if qid in needed_qids:\n",
    "            dev_queries[qid] = query.text\n",
    "\n",
    "    with open('data/top1000.dev', 'rt') as fIn:\n",
    "        for line in fIn:\n",
    "            qid, pid, query, passage = line.strip().split(\"\\t\")\n",
    "            corpus[pid] = passage\n",
    "            retrieved_docs[qid].add(pid)\n",
    "\n",
    "\n",
    "    ## Run evaluator\n",
    "    print(\"Queries: {}\".format(len(dev_queries)))\n",
    "\n",
    "    mrr_scores = []\n",
    "    hf_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for qid in tqdm.tqdm(dev_queries, total=len(dev_queries)):\n",
    "            query = dev_queries[qid]\n",
    "            top_pids = list(retrieved_docs[qid])\n",
    "            cross_inp = [[query, corpus[pid]] for pid in top_pids]\n",
    "\n",
    "            encoded = tokenizer(cross_inp, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "            output = model(**encoded)\n",
    "            bert_score = output.logits.detach().cpu().numpy()\n",
    "            bert_score = np.squeeze(bert_score)\n",
    "        \n",
    "            argsort = np.argsort(-bert_score)\n",
    "\n",
    "            rank_score = 0\n",
    "            for rank, idx in enumerate(argsort[0:10]):\n",
    "                pid = top_pids[idx]\n",
    "                if pid in dev_rel_docs[qid]:\n",
    "                    rank_score = 1/(rank+1)\n",
    "                    break\n",
    "\n",
    "            mrr_scores.append(rank_score)\n",
    "        \n",
    "            if len(mrr_scores) % 10 == 0:\n",
    "                print(\"{} MRR@10: {:.2f}\".format(len(mrr_scores), 100*np.mean(mrr_scores)))\n",
    "\n",
    "    print(\"MRR@10: {:.2f}\".format(np.mean(mrr_scores)*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--num_gpus NUM_GPUS]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--strategy STRATEGY] [--model MODEL] [--eval]\n",
      "                             [--ckpt CKPT]\n",
      "                             [--cross_lingual_chance CROSS_LINGUAL_CHANCE]\n",
      "                             [--precision PRECISION] [--num_negs NUM_NEGS]\n",
      "                             [--langs LANGS [LANGS ...]]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/avd6/.local/share/jupyter/runtime/kernel-c5ebf186-8b76-4d30-be2b-55ad3db225e6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--num_gpus\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--strategy\", default=None)\n",
    "    parser.add_argument(\"--model\", default='facebook/contriever-msmarco')\n",
    "    parser.add_argument(\"--eval\", action=\"store_true\")\n",
    "    parser.add_argument(\"--ckpt\")\n",
    "    parser.add_argument(\"--cross_lingual_chance\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--precision\", type=int, default=16)\n",
    "    parser.add_argument(\"--num_negs\", type=int, default=3)\n",
    "    parser.add_argument(\"--langs\", nargs=\"+\", default=['english']) #, 'chinese', 'french', 'german', 'indonesian', 'italian', 'portuguese', 'russian', 'spanish', 'arabic', 'dutch', 'hindi', 'japanese', 'vietnamese'\n",
    "    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.eval:\n",
    "        eval(args)\n",
    "    else:\n",
    "        main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f84259a3b330308de677e904dd70af8e4d9960a0b20e6e297421ba1b1b840763"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
