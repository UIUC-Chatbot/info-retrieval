{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72a61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8074279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the class\n",
    "\n",
    "class ContrieverCB:\n",
    "    def __init__(self):\n",
    "        self.embeddings = {}\n",
    "    \n",
    "    \n",
    "    def clean(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Function to remove newline from text.\n",
    "        :param text: input string\n",
    "        :return: string without newline\n",
    "        \"\"\"\n",
    "        new_text = re.sub('\\n', '', text)\n",
    "        return new_text\n",
    "    \n",
    "    \n",
    "    def mean_pooling(self, token_embeddings, mask):\n",
    "        \"\"\"\n",
    "        Function to be used after model is applied to tokenized text to generate embeddings.\n",
    "        Used in the HuggingFace version.\n",
    "        :param token_embeddings: output of model\n",
    "        :param mask: attention mask of the tokens\n",
    "        :return: tensors of the text\n",
    "        \"\"\"\n",
    "        token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    \n",
    "    def generate_embeddings(self, path_to_json: str, path_to_output: str) -> None:\n",
    "        \"\"\"\n",
    "        Function takes input json filepath, generates numpy embeddings of the file and\n",
    "        saves them at the given output filepath.\n",
    "        :param path_to_json: input filepath\n",
    "        :param path_to_output: output filepath\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "        model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "        \n",
    "        # open and read the input json file\n",
    "        file = open(path_to_json)\n",
    "        json_data = json.load(file)\n",
    "        \n",
    "        n = int(len(json_data)/100)\n",
    "        embeddings_list = []\n",
    "        \n",
    "        # take 100 units at a time and process it\n",
    "        for k in range(n):\n",
    "            if k==n:\n",
    "                start = k*100\n",
    "                end = (list(json_data.keys())[-1])\n",
    "            else:\n",
    "                start = k*100\n",
    "                end = k*100+99\n",
    "                \n",
    "            for i in range(start, end):\n",
    "                text = json_data[str(i)]\n",
    "                text = self.clean(text)\n",
    "                tokenized_text = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "                output = model(**tokenized_text)\n",
    "                embeddings = self.mean_pooling(output[0], tokenized_text['attention_mask'])\n",
    "                embeddings_np = embeddings.detach().numpy()\n",
    "                embeddings_list.append(embeddings_np)\n",
    "                \n",
    "        # convert embeddings list to numpy array\n",
    "        embeddings_array = np.array(embeddings_list)\n",
    "        \n",
    "        # reshape the numpy array\n",
    "        x = embeddings_array.shape[0]\n",
    "        y = embeddings_array.shape[2]\n",
    "        embeddings_array.reshape((x,y))\n",
    "        \n",
    "        # save the embeddings in a numpy file\n",
    "        # filename = path_to_json.split('\\\\')[-1].split('.')[0]\n",
    "        # filepath = os.path.join(path_to_output, filename)\n",
    "        filepath = path_to_output\n",
    "        \n",
    "        # saving the embeddings as a numpy file in the destination folder\n",
    "        np.save(filepath, embeddings_array)\n",
    "        \n",
    "        # saving the embeddings into a dictionary\n",
    "        self.embeddings[path_to_json] = embeddings_array\n",
    "        \n",
    "    \n",
    "    def retrieve_topk(self, search_string: str, path_to_json: str, k: int):\n",
    "        \"\"\"\n",
    "        Function takes json data as input and returns the topk relative to the data\n",
    "        :param search_string: query to match and retrieve\n",
    "        :param path_to_json: filepath of data to search\n",
    "        :param k: number of embeddings to retrieve\n",
    "        :return: top k units relative to the input file\n",
    "        \"\"\"\n",
    "        \n",
    "        # check if the embeddings are loaded in dictionary already\n",
    "        if not (path_to_json in self.embeddings):\n",
    "            \n",
    "            # changing .json to .npy\n",
    "            filename = os.path.splitext(path_to_json)[0]\n",
    "            path_to_npy = filename + '.npy'\n",
    "            \n",
    "            # check if .npy exists and load into dictionary\n",
    "            if os.path.exists(path_to_npy):\n",
    "                self.embeddings[path_to_json] = np.load(path_to_npy)\n",
    "            else:\n",
    "                # .npy doesn't exist, so generate embeddings\n",
    "                self.generate_embeddings(path_to_json, path_to_npy)\n",
    "        \n",
    "        # convert numpy embeddings to tensors\n",
    "        embeddings = torch.from_numpy(self.embeddings[path_to_json])\n",
    "        \n",
    "        # convert query to tensor\n",
    "        tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "        model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "        \n",
    "        tokenized_query = tokenizer(search_string, padding=True, truncation=True, return_tensors='pt')\n",
    "        output_query = model(**tokenized_query)\n",
    "        embedded_query = self.mean_pooling(output_query[0], tokenized_query['attention_mask'])\n",
    "        \n",
    "        # creating a dictionary of scores\n",
    "        scores_list = []\n",
    "        for i in range(len(embeddings)):\n",
    "            score = embeddings[i]@embedded_query[0]\n",
    "            score_np = score.detach().numpy()[0]\n",
    "            scores_list.append([i, score_np])\n",
    "            \n",
    "        scores_df = pd.DataFrame(scores_list, columns=['ID', 'Score'])\n",
    "        \n",
    "        # retrieving top k scores\n",
    "        topk_scores = scores_df.nlargest(k, 'Score')\n",
    "        \n",
    "        # retrieving the text data corresponding to the top k indices\n",
    "        topk_context = {}\n",
    "        \n",
    "        json_file = open(path_to_json)\n",
    "        json_data = json.load(json_file)\n",
    "        \n",
    "        for row in topk_scores.iterrows():\n",
    "            ind = int(row[1]['ID'])\n",
    "            text = json_data[str(ind)]\n",
    "            topk_context[ind] = text\n",
    "        \n",
    "        return topk_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dcfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8 (default, Nov 16 2020, 16:55:22) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
