{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ac1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05da11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422f0c55ad8c43fc98b8a0026714d6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb32112a28948f1bbd81c57ecdfa9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750ea0e87c7349bd8f944cdd5414ccba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0447f5bb22164ce6b258f12db22ef5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e1733ac6b44f7c8404d209cbe7b4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "model = AutoModel.from_pretrained('facebook/contriever-msmarco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7979748",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Where was Marie Curie born?\",\n",
    "    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n",
    "    \"Born in Paris on 15 May 1859, Pierre Curie was the son of EugÃ¨ne Curie, a doctor of French Catholic origin from Alsace.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46afadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer\n",
    "\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76312027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token embeddings\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba4f6668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1066,  0.0164,  0.0547,  ..., -0.0065, -0.0631, -0.0280],\n",
       "         [ 0.0192, -0.0602,  0.0502,  ...,  0.1185, -0.0204, -0.0111],\n",
       "         [ 0.1103, -0.0207,  0.0044,  ...,  0.0599, -0.0432,  0.0183],\n",
       "         ...,\n",
       "         [ 0.0305, -0.2994, -0.0895,  ...,  0.2829,  0.0329,  0.0951],\n",
       "         [ 0.0298, -0.3350, -0.0862,  ...,  0.2983,  0.0351,  0.0803],\n",
       "         [ 0.0299, -0.2928, -0.0951,  ...,  0.2991,  0.0265,  0.0623]],\n",
       "\n",
       "        [[ 0.0799,  0.0201,  0.0418,  ...,  0.0752,  0.0130,  0.0336],\n",
       "         [-0.0250,  0.0173,  0.0594,  ...,  0.1173, -0.1175,  0.0543],\n",
       "         [ 0.0280,  0.0259, -0.0916,  ..., -0.1142, -0.0608,  0.1254],\n",
       "         ...,\n",
       "         [ 0.0315, -0.1623, -0.0734,  ...,  0.1735, -0.0453,  0.0775],\n",
       "         [ 0.0674, -0.1745, -0.0764,  ...,  0.1819, -0.0478,  0.0687],\n",
       "         [ 0.0397, -0.1430, -0.0823,  ...,  0.1684, -0.0414,  0.0401]],\n",
       "\n",
       "        [[ 0.0364, -0.0647,  0.0651,  ...,  0.0903,  0.0586,  0.0269],\n",
       "         [-0.0173,  0.2125,  0.0231,  ..., -0.0572,  0.0988, -0.2662],\n",
       "         [-0.0478, -0.1174, -0.1090,  ...,  0.0656,  0.0384, -0.0421],\n",
       "         ...,\n",
       "         [ 0.0904,  0.0055, -0.0863,  ...,  0.0272, -0.0274,  0.0982],\n",
       "         [-0.1369, -0.0950,  0.0118,  ..., -0.0915, -0.0675, -0.0088],\n",
       "         [-0.0762, -0.0748, -0.0086,  ...,  0.0026, -0.0144, -0.0229]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.0135, -0.0092,  0.0013,  ...,  0.0288,  0.0191, -0.0925],\n",
       "        [ 0.0191, -0.0188, -0.0401,  ...,  0.0127, -0.0204, -0.0245],\n",
       "        [ 0.0534, -0.0158, -0.1317,  ..., -0.0812, -0.0167, -0.0901]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de81239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pooling\n",
    "\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7bab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = mean_pooling(outputs[0], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d800add8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0161,  0.0055,  0.0199,  ...,  0.0372, -0.0831, -0.0112],\n",
       "        [ 0.0037,  0.0346, -0.0131,  ...,  0.0247, -0.1021, -0.0303],\n",
       "        [-0.0146, -0.0235, -0.0338,  ...,  0.0277, -0.0025, -0.0092]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f9e44e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2073,  2001,  5032, 12731,  7373,  2141,  1029,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  3814, 15315,  4135,  3527,  9333,  2912,  1010,  2101,  2124,\n",
       "          2004,  5032, 12731,  7373,  1010,  2001,  2141,  2006,  2281,  1021,\n",
       "          1010,  7517,  1012,   102,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2141,  1999,  3000,  2006,  2321,  2089,  8165,  1010,  5578,\n",
       "         12731,  7373,  2001,  1996,  2365,  1997,  8207, 12731,  7373,  1010,\n",
       "          1037,  3460,  1997,  2413,  3234,  4761,  2013, 24922,  1012,   102]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78291dfa",
   "metadata": {},
   "source": [
    "Work on generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75ff9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"all_data.jl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13f2d5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt-3</th>\n",
       "      <th>gpt-3_paragraphs</th>\n",
       "      <th>end2end</th>\n",
       "      <th>fine_tuned_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'text': 'Title: Finite State Machine Design ...</td>\n",
       "      <td>[{'text': 'Title: The Halting Problem P0 \n",
       " Tex...</td>\n",
       "      <td>[{'text': 'Title: Finite State Machine Design ...</td>\n",
       "      <td>[{'text': 'Title: Finite State Machine Design ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               gpt-3  \\\n",
       "0  [{'text': 'Title: Finite State Machine Design ...   \n",
       "\n",
       "                                    gpt-3_paragraphs  \\\n",
       "0  [{'text': 'Title: The Halting Problem P0 \n",
       " Tex...   \n",
       "\n",
       "                                             end2end  \\\n",
       "0  [{'text': 'Title: Finite State Machine Design ...   \n",
       "\n",
       "                                     fine_tuned_data  \n",
       "0  [{'text': 'Title: Finite State Machine Design ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8b6d312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Halting Problem P1 \n",
      " Text: In order to make these notes more useful as a reference, definitions are\n",
      "highlighted with boldface, and italicization emphasizes pitfalls or other\n",
      "important points. \n",
      " Question: \n",
      "Q. What is the difference between a pitfall and an important point? \n",
      " Answer: \n",
      "\n",
      "A. A pitfall is a potential problem or danger that could occur, while an important point is something that is noteworthy or deserves attention.\n"
     ]
    }
   ],
   "source": [
    "print(data['gpt-3_paragraphs'][0][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7047fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Steps in the Design Process \n",
      " Text: {Steps in the Design Process}\n",
      "\n",
      "Before we begin exploring designs, let's talk briefly about the general\n",
      "approach that we take when designing an FSM.  We follow a six-step\n",
      "process:{-8pt}\n",
      "\n",
      "{{}{}\n",
      "{}{}{}\n",
      "{develop an abstract model}{step-abs}\n",
      "{specify I/O behavior}{step-io}\n",
      "{complete the specification}{step-complete}\n",
      "{choose a state representation}{step-repn}\n",
      "{calculate logic expressions}{step-logic}\n",
      "{implement with flip-flops and gates}{step-gates}\n",
      "}\n",
      "{-8pt}\n",
      "\n",
      "In Step {step-abs}, we translate our description in human language\n",
      "into a model with states and desired behavior.  At this stage, we \n",
      "simply try to capture the intent of the description and are not\n",
      "particularly thorough nor exact.\n",
      "\n",
      "Step {step-io} begins to formalize the model, starting with its\n",
      "input and output behavior.  If we eventually plan to develop an\n",
      "implementation of our FSM as a digital system (which is not the \n",
      "only choice, of course!), all input and output\n",
      "must consist of bits.  Often, input and/or output specifications\n",
      "may need to match other digital systems to which we plan to connect\n",
      "our FSM.  In fact, { most problems in developing large digital systems\n",
      "today arise because of incompatibilities when composing two or more\n",
      "separately designed pieces} (or { modules}) into an integrated system.\n",
      "\n",
      "Once we know the I/O behavior for our FSM, in Step {step-complete}\n",
      "we start to make\n",
      "any implicit assumptions clear and to make any other decisions\n",
      "necessary to the design.  Occasionally, we may choose to leave\n",
      "something undecided in the hope of simplifying the design with\n",
      "``don't care'' entries in the logic formulation.\n",
      "\n",
      "In Step {step-repn}, we select an internal representation\n",
      "for the bits necessary to encode the state of our FSM.  In practice,\n",
      "for small designs, this representation can be selected by a computer \n",
      "in such a way as to optimize the implementation.  However, for large\n",
      "designs, such as the LC-3 instruction set architecture that we\n",
      "study later in this class, humans do most of the work by hand.\n",
      "\n",
      "In the later examples in this set of notes, we show how even a \n",
      "small design can\n",
      "leverage meaningful information from the design when selecting\n",
      "the representation, leading to an implementation that is simpler\n",
      "and is easier to build correctly.\n",
      "\n",
      "We also show how one can\n",
      "use abstraction to simplify an implementation.\n",
      "\n",
      "By Step {step-logic}, our design is a complete specification in\n",
      "terms of bits, and we need merely derive logic expressions for the\n",
      "next-state variables and the output signals.  This process is no\n",
      "different than for combinational logic, and should already be fairly \n",
      "familiar to you.\n",
      "\n",
      "\n",
      "\n",
      "Finally, in Step {step-gates}, we translate our logic expressions\n",
      "into gates and use flip-flops (or registers) to hold the internal\n",
      "state bits of the FSM.  In later notes, we use more complex\n",
      "building blocks when implementing an FSM, building up abstractions\n",
      "in order to simplify the design process in much the same way that\n",
      "we have shown for combinational logic.\n",
      "\n",
      "\n",
      " \n",
      " Question: Why do most problems arise in developing large digital systems today? \n",
      " Answer: \n",
      "\n",
      "A. Most problems in developing large digital systems today arise because of incompatibilities when composing two or more\n",
      "separately designed pieces into an integrated system.\n"
     ]
    }
   ],
   "source": [
    "print(data['fine_tuned_data'][0][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5c9ca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Steps in the Design Process \n",
      " Text: {Steps in the Design Process}\n",
      "\n",
      "Before we begin exploring designs, let's talk briefly about the general\n",
      "approach that we take when designing an FSM.  We follow a six-step\n",
      "process:{-8pt}\n",
      "\n",
      "{{}{}\n",
      "{}{}{}\n",
      "{develop an abstract model}{step-abs}\n",
      "{specify I/O behavior}{step-io}\n",
      "{complete the specification}{step-complete}\n",
      "{choose a state representation}{step-repn}\n",
      "{calculate logic expressions}{step-logic}\n",
      "{implement with flip-flops and gates}{step-gates}\n",
      "}\n",
      "{-8pt}\n",
      "\n",
      "In Step {step-abs}, we translate our description in human language\n",
      "into a model with states and desired behavior.  At this stage, we \n",
      "simply try to capture the intent of the description and are not\n",
      "particularly thorough nor exact.\n",
      "\n",
      "Step {step-io} begins to formalize the model, starting with its\n",
      "input and output behavior.  If we eventually plan to develop an\n",
      "implementation of our FSM as a digital system (which is not the \n",
      "only choice, of course!), all input and output\n",
      "must consist of bits.  Often, input and/or output specifications\n",
      "may need to match other digital systems to which we plan to connect\n",
      "our FSM.  In fact, { most problems in developing large digital systems\n",
      "today arise because of incompatibilities when composing two or more\n",
      "separately designed pieces} (or { modules}) into an integrated system.\n",
      "\n",
      "Once we know the I/O behavior for our FSM, in Step {step-complete}\n",
      "we start to make\n",
      "any implicit assumptions clear and to make any other decisions\n",
      "necessary to the design.  Occasionally, we may choose to leave\n",
      "something undecided in the hope of simplifying the design with\n",
      "``don't care'' entries in the logic formulation.\n",
      "\n",
      "In Step {step-repn}, we select an internal representation\n",
      "for the bits necessary to encode the state of our FSM.  In practice,\n",
      "for small designs, this representation can be selected by a computer \n",
      "in such a way as to optimize the implementation.  However, for large\n",
      "designs, such as the LC-3 instruction set architecture that we\n",
      "study later in this class, humans do most of the work by hand.\n",
      "\n",
      "In the later examples in this set of notes, we show how even a \n",
      "small design can\n",
      "leverage meaningful information from the design when selecting\n",
      "the representation, leading to an implementation that is simpler\n",
      "and is easier to build correctly.\n",
      "\n",
      "We also show how one can\n",
      "use abstraction to simplify an implementation.\n",
      "\n",
      "By Step {step-logic}, our design is a complete specification in\n",
      "terms of bits, and we need merely derive logic expressions for the\n",
      "next-state variables and the output signals.  This process is no\n",
      "different than for combinational logic, and should already be fairly \n",
      "familiar to you.\n",
      "\n",
      "\n",
      "\n",
      "Finally, in Step {step-gates}, we translate our logic expressions\n",
      "into gates and use flip-flops (or registers) to hold the internal\n",
      "state bits of the FSM.  In later notes, we use more complex\n",
      "building blocks when implementing an FSM, building up abstractions\n",
      "in order to simplify the design process in much the same way that\n",
      "we have shown for combinational logic.\n",
      "\n",
      "\n",
      " \n",
      " Question: Why do most problems arise in developing large digital systems today? \n",
      " Answer: \n",
      "\n",
      "A. Most problems in developing large digital systems today arise because of incompatibilities when composing two or more\n",
      "separately designed pieces into an integrated system.\n"
     ]
    }
   ],
   "source": [
    "print(data['end2end'][0][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bea02395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title: Steps in the Design Process \\n Text: {Steps in the Design Process}\\n\\nBefore we begin exploring designs, let's talk briefly about the general\\napproach that we take when designing an FSM.  We follow a six-step\\nprocess:{-8pt}\\n\\n{{}{}\\n{}{}{}\\n{develop an abstract model}{step-abs}\\n{specify I/O behavior}{step-io}\\n{complete the specification}{step-complete}\\n{choose a state representation}{step-repn}\\n{calculate logic expressions}{step-logic}\\n{implement with flip-flops and gates}{step-gates}\\n}\\n{-8pt}\\n\\nIn Step {step-abs}, we translate our description in human language\\ninto a model with states and desired behavior.  At this stage, we \\nsimply try to capture the intent of the description and are not\\nparticularly thorough nor exact.\\n\\nStep {step-io} begins to formalize the model, starting with its\\ninput and output behavior.  If we eventually plan to develop an\\nimplementation of our FSM as a digital system (which is not the \\nonly choice, of course!), all input and output\\nmust consist of bits.  Often, input and/or output specifications\\nmay need to match other digital systems to which we plan to connect\\nour FSM.  In fact, { most problems in developing large digital systems\\ntoday arise because of incompatibilities when composing two or more\\nseparately designed pieces} (or { modules}) into an integrated system.\\n\\nOnce we know the I/O behavior for our FSM, in Step {step-complete}\\nwe start to make\\nany implicit assumptions clear and to make any other decisions\\nnecessary to the design.  Occasionally, we may choose to leave\\nsomething undecided in the hope of simplifying the design with\\n``don't care'' entries in the logic formulation.\\n\\nIn Step {step-repn}, we select an internal representation\\nfor the bits necessary to encode the state of our FSM.  In practice,\\nfor small designs, this representation can be selected by a computer \\nin such a way as to optimize the implementation.  However, for large\\ndesigns, such as the LC-3 instruction set architecture that we\\nstudy later in this class, humans do most of the work by hand.\\n\\nIn the later examples in this set of notes, we show how even a \\nsmall design can\\nleverage meaningful information from the design when selecting\\nthe representation, leading to an implementation that is simpler\\nand is easier to build correctly.\\n\\nWe also show how one can\\nuse abstraction to simplify an implementation.\\n\\nBy Step {step-logic}, our design is a complete specification in\\nterms of bits, and we need merely derive logic expressions for the\\nnext-state variables and the output signals.  This process is no\\ndifferent than for combinational logic, and should already be fairly \\nfamiliar to you.\\n\\n\\n\\nFinally, in Step {step-gates}, we translate our logic expressions\\ninto gates and use flip-flops (or registers) to hold the internal\\nstate bits of the FSM.  In later notes, we use more complex\\nbuilding blocks when implementing an FSM, building up abstractions\\nin order to simplify the design process in much the same way that\\nwe have shown for combinational logic.\\n\\n\\n \\n Question: Why do most problems arise in developing large digital systems today? \\n Answer: \\n\\nA. Most problems in developing large digital systems today arise because of incompatibilities when composing two or more\\nseparately designed pieces into an integrated system.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['end2end'][0][1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872220c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to clean data and organize in a proper dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to verify?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
