{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers (specifically stsb-mpnet-base-v2)\n",
    "This is THE BEST sentence-level embedding model on huggingface. \n",
    "But we'll see if it's good enough for the real world. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.shape: (3, 768)\n",
      "5.535633 0.17808995\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\", \"Kastan is a fun programmer\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/stsb-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "# print(embeddings)\n",
    "print(\"embeddings.shape:\", embeddings.shape)\n",
    "\n",
    "score01 = embeddings[0] @ embeddings[1] #1.0473\n",
    "score02 = embeddings[0] @ embeddings[2] #1.0095\n",
    "# score02 = embeddings[0] @ embeddings[3] #1.0095\n",
    "\n",
    "print(score01, score02) # the first two are closer than the first and third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc Query\n",
    "This does pure question to text lookup (no generation).\n",
    "But I like that because hopefully it's more factual. \n",
    "\n",
    "Also this implementation works directly with PDFs! That's awesome for easily using all kinds of new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docquery import document, pipeline\n",
    "p = pipeline('document-question-answering')\n",
    "\n",
    "# using shorter PDF for quick testing\n",
    "doc = document.load_document(\"../notes/Student_Notes_short.pdf\")\n",
    "\n",
    "# use full pdf for real testing\n",
    "# doc = document.load_document(\"../data-generator/notes/Student Notes.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are boolean logic operations? {'score': 0.9949806332588196, 'answer': 'notational conventions and tools that we use to express general functions on bits.', 'word_ids': [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], 'page': 10}\n",
      "What is overflow? {'score': 0.04521600529551506, 'answer': 'negative pattern for -8 in 4-bit 2’s complement.', 'word_ids': [134, 135, 136, 137, 138, 139, 140, 141], 'page': 5}\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "  \"What are boolean logic operations?\", \n",
    "  \"What is overflow?\",\n",
    "  ]\n",
    "\n",
    "for q in questions:\n",
    "  print(q, p(question=q, **doc.context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "document-question-answering is already registered. Overwriting pipeline for task document-question-answering...\n",
      "image-classification is already registered. Overwriting pipeline for task image-classification...\n",
      "2022-10-01 15:35:02,786 INFO: Loading https://templates.invoicehome.com/invoice-template-us-neat-750px.png\n",
      "2022-10-01 15:35:02,786 INFO: Done loading files. Loading pipeline...\n",
      "Downloading: 100%|██████████████████████████████| 789/789 [00:00<00:00, 227kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 315/315 [00:00<00:00, 112kB/s]\n",
      "Downloading: 100%|███████████████████████████| 798k/798k [00:00<00:00, 1.98MB/s]\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 2.44MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.36M/1.36M [00:00<00:00, 7.44MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 239/239 [00:00<00:00, 103kB/s]\n",
      "Downloading: 100%|███████████████████████████| 511M/511M [00:38<00:00, 13.2MB/s]\n",
      "2022-10-01 15:35:47,045 INFO: Ready to start evaluating!\n",
      "https://templates.invoicehome.com/invoice-template-us-neat-750px.png What is the invoice number?: us-001\n"
     ]
    }
   ],
   "source": [
    "!docquery scan \"What is the invoice number?\" https://templates.invoicehome.com/invoice-template-us-neat-750px.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 249kB/s]\n",
      "Downloading: 100%|██████████| 496M/496M [00:27<00:00, 18.3MB/s] \n",
      "Downloading: 100%|██████████| 79.0/79.0 [00:00<00:00, 37.3kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 2.82MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 1.83MB/s] \n",
      "Downloading: 100%|██████████| 772/772 [00:00<00:00, 288kB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "with open('../gpt-3/GPT-3_section_level.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# top_k sets how many answers you want the pipeline to return (each with a score)\n",
    "# model = SentenceTransformer('sentence-transformers/stsb-mpnet-base-v2')\n",
    "model = \"deepset/roberta-base-squad2\"\n",
    "pipe = pipeline('question-answering', model=model, tokenizer=model, max_answer_len=128, top_k=5)\n",
    "all_retrieve_data = []\n",
    "for i in range(len(data)):\n",
    "    question = re.sub('\\nQ.', '', data[i]['questions'])\n",
    "    context = re.sub('\\n', ' ', data[i]['positive_ctxs']['text'])\n",
    "    if not question or not context:\n",
    "        continue\n",
    "    retrieval = pipe(question=question, context=context)\n",
    "    all_retrieve_data.append(retrieval)\n",
    "\n",
    "with open('section_level_retrieval.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_retrieve_data, f, ensure_ascii=False, indent=4) \n",
    "\n",
    "# Returns something like this:\n",
    "# [{'score': 0.47350358963012695, 'start': 20, 'end': 28, 'answer': 'textbook'},\n",
    "#  {'score': 0.1505853682756424,\n",
    "#   'start': 20,\n",
    "#   'end': 41,\n",
    "#   'answer': 'textbook and in class'},\n",
    "#  {'score': 0.041666436940431595,\n",
    "#   'start': 16,\n",
    "#   'end': 28,\n",
    "#   'answer': 'the textbook'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('as1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa3aaf629b57a769da776a59de5d566730fc031530e023c4ad5d0c622cc546f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
