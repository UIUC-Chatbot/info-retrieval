{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers (specifically stsb-mpnet-base-v2)\n",
    "This is THE BEST sentence-level embedding model on huggingface. \n",
    "But we'll see if it's good enough for the real world. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "embeddings.shape: (3, 768)\n",
      "5.535631 0.17808935\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\", \"Kastan is a fun programmer\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/stsb-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "# print(embeddings)\n",
    "print(\"embeddings.shape:\", embeddings.shape)\n",
    "\n",
    "score01 = embeddings[0] @ embeddings[1] #1.0473\n",
    "score02 = embeddings[0] @ embeddings[2] #1.0095\n",
    "# score02 = embeddings[0] @ embeddings[3] #1.0095\n",
    "\n",
    "print(score01, score02) # the first two are closer than the first and third"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc Query\n",
    "This does pure question to text lookup (no generation).\n",
    "But I like that because hopefully it's more factual. \n",
    "\n",
    "Also this implementation works directly with PDFs! That's awesome for easily using all kinds of new data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docquery import document, pipeline\n",
    "import json\n",
    "import re\n",
    "\n",
    "class DocQuery():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def query(self, data, top_k):\n",
    "        p = pipeline('document-question-answering')\n",
    "        doc = document.load_document(\"../Student_Notes.pdf\")\n",
    "        questions = []\n",
    "        for i in range(len(data)):\n",
    "            question = re.sub('\\nQ.', '', data[i]['questions'])\n",
    "            questions.append(question)\n",
    "\n",
    "        all_data = []\n",
    "        for q in questions:\n",
    "        # print(q, p(question=q, **doc.context))\n",
    "            answer = p(question=q, **doc.context, top_k=top_k)\n",
    "            data = {}\n",
    "            data['questions'] = q\n",
    "            data['answers'] = answer\n",
    "            all_data.append(data)\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the DocQuery class\n",
    "d = DocQuery()\n",
    "with open('../GPT-3_section_level.json') as f:  # use the questions in section data as query\n",
    "    query = json.load(f)\n",
    "all_data = d.query(query, 3)\n",
    "with open('docquery_section_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docquery scan \"What is the invoice number?\" https://templates.invoicehome.com/invoice-template-us-neat-750px.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Wiki Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retreive answers from given context (Retrieval Pipeline)\n",
    "import re\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "class WikiRetrieval(): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def retrieval_pipeline(self):\n",
    "        # retrieve top 5 answers from given context and question \n",
    "        with open('GPT-3_paragraph_level.json') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # top_k sets how many answers you want the pipeline to return (each with a score)\n",
    "        model = \"deepset/roberta-base-squad2\"\n",
    "        pipe = pipeline('question-answering', model=model, tokenizer=model, max_answer_len=128, top_k=5)\n",
    "        all_retrieve_data = []\n",
    "        for i in range(len(data)):\n",
    "            question = re.sub('\\nQ.', '', data[i]['questions'])\n",
    "            context = re.sub('\\n', ' ', data[i]['positive_ctxs']['text'])\n",
    "            if not question or not context:\n",
    "                continue\n",
    "            retrieval = pipe(question=question, context=context)\n",
    "            all_retrieve_data.append(retrieval)\n",
    "\n",
    "        # with open('section_level_retrieval.json', 'w', encoding='utf-8') as f:\n",
    "        #     json.dump(all_retrieve_data, f, ensure_ascii=False, indent=4) \n",
    "        # Returns something like this:\n",
    "        # [{'score': 0.47350358963012695, 'start': 20, 'end': 28, 'answer': 'textbook'},\n",
    "        #  {'score': 0.1505853682756424,\n",
    "        #   'start': 20,\n",
    "        #   'end': 41,\n",
    "        #   'answer': 'textbook and in class'},\n",
    "        #  {'score': 0.041666436940431595,\n",
    "        #   'start': 16,\n",
    "        #   'end': 28,\n",
    "        #   'answer': 'the textbook'}]\n",
    "        return all_retrieve_data\n",
    "\n",
    "\n",
    "    \n",
    "    def search(self, query):\n",
    "    # This function will search all wikipedia articles for passages that\n",
    "    # answer the query\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "        #We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "        bi_encoder = SentenceTransformer('sentence-transformers/stsb-mpnet-base-v2')\n",
    "        bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "        top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "        #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "        # As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
    "        # about 170k articles. We split these articles into paragraphs and encode them with the bi-encoder\n",
    "\n",
    "        wikipedia_filepath = 'simplewiki-2020-11-01.jsonl.gz'\n",
    "\n",
    "        if not os.path.exists(wikipedia_filepath):\n",
    "            util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
    "\n",
    "        passages = []\n",
    "        with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "            for line in fIn:\n",
    "                data = json.loads(line.strip())\n",
    "\n",
    "                #Add all paragraphs\n",
    "                #passages.extend(data['paragraphs'])\n",
    "\n",
    "                #Only add the first paragraph\n",
    "                passages.append(data['paragraphs'][0])\n",
    "\n",
    "        print(\"Passages:\", len(passages))\n",
    "\n",
    "        # We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "        corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "        \n",
    "        ##### Sematic Search #####\n",
    "        # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "        question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "        question_embedding = question_embedding.cuda()\n",
    "        hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "        hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "        ##### Re-Ranking #####\n",
    "        # Now, score all retrieved passages with the cross_encoder\n",
    "        cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "        cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "        # Sort results by the cross-encoder scores\n",
    "        for idx in range(len(cross_scores)):\n",
    "            hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "        # Output of top-5 hits from re-ranker\n",
    "        hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "        output = []\n",
    "        for hit in hits[0:5]:\n",
    "            temp = passages[hit['corpus_id']].replace(\"\\n\", \" \")\n",
    "            output.append(temp)\n",
    "        data = {}\n",
    "        data[query] = output\n",
    "        return data\n",
    "    \n",
    "    def search_wiki(self, data):\n",
    "        # This function will search all wikipedia articles for passages that answer the query\n",
    "        all_retrieve_data = []\n",
    "        for i in range(len(data)):\n",
    "            question = re.sub('\\nQ.', '', data[i]['questions'])\n",
    "            retrieved_passage = self.search(query=question)\n",
    "            if not question:\n",
    "                continue\n",
    "            all_retrieve_data.append(retrieved_passage)\n",
    "\n",
    "        return all_retrieve_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the WikiRetrieval class\n",
    "r = WikiRetrieval()\n",
    "with open('GPT-3_paragraph_level.json') as f:\n",
    "    query = json.load(f)\n",
    "all_retrieve_data = r.search_wiki(query)\n",
    "with open('wiki_retrieval_paragraph.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_retrieve_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph Retrieval (Cross-Encoder Re-Ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive answers from given context (Retrieval Pipeline)\n",
    "import re\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "class ParagraphRetrieval(): \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def clean(self, text):\n",
    "        new_text = re.sub('\\n', '', text)\n",
    "        return new_text\n",
    "    \n",
    "    def search(self, query):\n",
    "        # This function will search all wikipedia articles for passages that\n",
    "        # answer the query\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"Warning: No GPU found. Please add GPU to your notebook\")\n",
    "\n",
    "        bi_encoder = SentenceTransformer('sentence-transformers/stsb-mpnet-base-v2')\n",
    "        bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "        top_k = 32                          #Number of passages we want to retrieve with the bi-encoder\n",
    "\n",
    "        #The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "        # We use paragraphs.json as the retrieval dataset\n",
    "        passages = []\n",
    "        with open('../paragraphs.json') as f:\n",
    "            data = json.load(f)\n",
    "            n = int(len(data)/100)\n",
    "            for k in range(n):\n",
    "                if(k==n):\n",
    "                    start = k*100\n",
    "                    end = (list(data.keys())[-1])\n",
    "                else:\n",
    "                    start = k*100\n",
    "                    end = k*100+99\n",
    "                for i in range(start, end):\n",
    "                    paragraph = data[str(i)]\n",
    "                    paragraph = self.clean(paragraph)\n",
    "                    passages.append(paragraph)\n",
    "\n",
    "        print(\"Passages:\", len(passages))\n",
    "\n",
    "        # We encode all passages into our vector space. This takes about 5 minutes (depends on your GPU speed)\n",
    "        corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "        ##### Sematic Search #####\n",
    "        # Encode the query using the bi-encoder and find potentially relevant passages\n",
    "        question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "        question_embedding = question_embedding.cuda()\n",
    "        hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "        hits = hits[0]  # Get the hits for the first query\n",
    "\n",
    "        ##### Re-Ranking #####\n",
    "        # Now, score all retrieved passages with the cross_encoder\n",
    "        cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "        cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "        # Sort results by the cross-encoder scores\n",
    "        for idx in range(len(cross_scores)):\n",
    "            hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "\n",
    "        # Output of top-5 hits from re-ranker\n",
    "    #     print(\"\\n-------------------------\\n\")\n",
    "    #     print(\"Top-3 Cross-Encoder Re-ranker hits\")\n",
    "        hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "        output = []\n",
    "        data = {}\n",
    "        for hit in hits[0:5]:\n",
    "            temp = passages[hit['corpus_id']].replace(\"\\n\", \" \")\n",
    "            out = str(hit['cross-score']) + \" \" + temp\n",
    "            output.append(out)\n",
    "        data[query] = output\n",
    "        return data\n",
    "    \n",
    "    def search_paragraph(self, data):\n",
    "        # This function will search all wikipedia articles for passages that answer the query\n",
    "        all_retrieve_data = []\n",
    "        for i in range(len(data)):\n",
    "            question = re.sub('\\nQ.', '', data[i]['questions'])\n",
    "            retrieved_passage = self.search(query=question)\n",
    "            if not question:\n",
    "                continue\n",
    "            all_retrieve_data.append(retrieved_passage)\n",
    "\n",
    "        return all_retrieve_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the class\n",
    "p = ParagraphRetrieval()\n",
    "with open('../GPT-3_paragraph_level.json') as f:\n",
    "    query = json.load(f)\n",
    "all_retrieve_data = p.search_paragraph(query)\n",
    "with open('sentence_transformer_paragraph_cross_encoder.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_retrieve_data, f, ensure_ascii=False, indent=4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('as1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa3aaf629b57a769da776a59de5d566730fc031530e023c4ad5d0c622cc546f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
