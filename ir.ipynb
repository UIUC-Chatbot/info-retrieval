{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure you have necessary packages \n",
    "import transformers \n",
    "import datasets \n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "     ---------------------------------------- 4.9/4.9 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.9.13-cp310-cp310-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.7/267.7 kB 4.1 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.9.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "     -------------------------------------- 120.7/120.7 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.5/61.5 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.4/140.4 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, urllib3, typing-extensions, tqdm, regex, pyyaml, idna, filelock, charset-normalizer, requests, huggingface-hub, transformers\n",
      "Successfully installed charset-normalizer-2.1.1 filelock-3.8.0 huggingface-hub-0.9.1 idna-3.4 pyyaml-6.0 regex-2022.9.13 requests-2.28.1 tokenizers-0.12.1 tqdm-4.64.1 transformers-4.22.1 typing-extensions-4.3.0 urllib3-1.26.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.5.1-py3-none-any.whl (431 kB)\n",
      "     -------------------------------------- 431.2/431.2 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.0-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "     ---------------------------------------- 10.4/10.4 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp310-cp310-win_amd64.whl (319 kB)\n",
      "     -------------------------------------- 319.7/319.7 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.1/133.1 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets) (0.9.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets) (1.23.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp310-cp310-win_amd64.whl (19.5 MB)\n",
      "     ---------------------------------------- 19.5/19.5 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.8/140.8 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.8/95.8 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp310-cp310-win_amd64.whl (33 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp310-cp310-win_amd64.whl (55 kB)\n",
      "     -------------------------------------- 55.9/55.9 kB 972.3 kB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp310-cp310-win_amd64.whl (27 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
      "     -------------------------------------- 500.6/500.6 kB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pytz, xxhash, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.5.1 dill-0.3.5.1 frozenlist-1.3.1 fsspec-2022.8.2 multidict-6.0.2 multiprocess-0.70.13 pandas-1.5.0 pyarrow-9.0.0 pytz-2022.2.1 responses-0.18.0 xxhash-3.0.0 yarl-1.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beir\n",
      "  Downloading beir-1.0.1.tar.gz (50 kB)\n",
      "     ---------------------------------------- 50.3/50.3 kB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 86.0/86.0 kB 4.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pytrec_eval\n",
      "  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting faiss_cpu\n",
      "  Downloading faiss_cpu-1.7.2-cp310-cp310-win_amd64.whl (9.9 MB)\n",
      "     ---------------------------------------- 9.9/9.9 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting elasticsearch==7.9.1\n",
      "  Downloading elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
      "     -------------------------------------- 219.2/219.2 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: datasets in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from beir) (2.5.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from elasticsearch==7.9.1->beir) (2022.9.14)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from elasticsearch==7.9.1->beir) (1.26.12)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (9.0.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (1.23.1)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (0.3.5.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (3.8.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (21.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (1.5.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from datasets->beir) (2022.8.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from sentence-transformers->beir) (4.22.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from sentence-transformers->beir) (1.12.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 7.5 MB/s eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.2-cp310-cp310-win_amd64.whl (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.9.1-cp310-cp310-win_amd64.whl (38.6 MB)\n",
      "     ---------------------------------------- 38.6/38.6 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets->beir) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets->beir) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets->beir) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets->beir) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets->beir) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets->beir) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from aiohttp->datasets->beir) (1.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->beir) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->beir) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets->beir) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from packaging->datasets->beir) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from requests>=2.19.0->datasets->beir) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from tqdm>=4.62.1->datasets->beir) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->beir) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->beir) (0.12.1)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 298.0/298.0 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from pandas->datasets->beir) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from pandas->datasets->beir) (2.8.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.2.0-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets->beir) (1.16.0)\n",
      "Building wheels for collected packages: beir, pytrec_eval, sentence-transformers\n",
      "  Building wheel for beir (setup.py): started\n",
      "  Building wheel for beir (setup.py): finished with status 'done'\n",
      "  Created wheel for beir: filename=beir-1.0.1-py3-none-any.whl size=62501 sha256=a70163d9e2f1d64d9c589a1b322852700787ac5bed9e9c38b10279ed0d69fb62\n",
      "  Stored in directory: c:\\users\\dabho\\appdata\\local\\pip\\cache\\wheels\\db\\39\\65\\c3e53a900805045248fd6bed1b8ce0cc271bac7b3c9dd0f138\n",
      "  Building wheel for pytrec_eval (setup.py): started\n",
      "  Building wheel for pytrec_eval (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pytrec_eval\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=5b5710c47b64e74af3178402e3e7ef8b8e8e58ffb636a46d00c9fa818823a8ac\n",
      "  Stored in directory: c:\\users\\dabho\\appdata\\local\\pip\\cache\\wheels\\62\\f2\\10\\1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built beir sentence-transformers\n",
      "Failed to build pytrec_eval\n",
      "Installing collected packages: sentencepiece, faiss_cpu, threadpoolctl, scipy, pytrec_eval, pillow, joblib, elasticsearch, click, torchvision, scikit-learn, nltk, sentence-transformers, beir\n",
      "  Running setup.py install for pytrec_eval: started\n",
      "  Running setup.py install for pytrec_eval: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [13 lines of output]\n",
      "  Fetching trec_eval from https://github.com/usnistgov/trec_eval/archive/v9.0.8.tar.gz.\n",
      "  C:\\Users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\setuptools\\dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\pytrec_eval\n",
      "  copying py\\__init__.py -> build\\lib.win-amd64-cpython-310\\pytrec_eval\n",
      "  running build_ext\n",
      "  building 'pytrec_eval_ext' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pytrec_eval\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for pytrec_eval did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  Fetching trec_eval from https://github.com/usnistgov/trec_eval/archive/v9.0.8.tar.gz.\n",
      "  C:\\Users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\setuptools\\dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "    warnings.warn(\n",
      "  running install\n",
      "  C:\\Users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\pytrec_eval\n",
      "  copying py\\__init__.py -> build\\lib.win-amd64-cpython-310\\pytrec_eval\n",
      "  running build_ext\n",
      "  building 'pytrec_eval_ext' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "pytrec_eval\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "pip install beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.12.1-cp310-cp310-win_amd64.whl (162.2 MB)\n",
      "     -------------------------------------- 162.2/162.2 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dabho\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages (from torch) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try OPT-125m (smallest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, are you consciours? Can you talk to me?\n",
      "I'm not sure, but I'm sure I can talk to you\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "prompt = \"Hey, are you consciours? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "generated_sentence = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Contriever \n",
    "\n",
    "First clone the contriever repo into the current directory. \n",
    "\n",
    "When pushing, **do not** push this directory.\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:facebookresearch/contriever.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'contriever'...\n",
      "Host key verification failed.\n",
      "fatal: Could not read from remote repository.\n",
      "\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n"
     ]
    }
   ],
   "source": [
    "!git clone git@github.com:facebookresearch/contriever.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a725a9b3aafd4ea3882abc7266bfd817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Contriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#from src.contriever import Contriever\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mContriever\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/contriever\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/contriever\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#Load the associated tokenizer:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere was Marie Curie born?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBorn in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Contriever' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# print(sys.path)\n",
    "sys.path.append(\"./contriever\")\n",
    "\n",
    "from src.contriever import Contriever\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = Contriever.from_pretrained(\"facebook/contriever\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/contriever\") #Load the associated tokenizer:\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Where was Marie Curie born?\",\n",
    "    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n",
    "    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "embeddings = model(**inputs)\n",
    "\n",
    "\n",
    "score01 = embeddings[0] @ embeddings[1] #1.0473\n",
    "score02 = embeddings[0] @ embeddings[2] #1.0095\n",
    "\n",
    "print(score01)\n",
    "print(score02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Windows requires Developer Mode to be activated, or to run Python as an administrator, in order to create symlinks.\nIn order to activate Developer Mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\huggingface_hub\\file_download.py:837\u001b[0m, in \u001b[0;36m_create_relative_symlink\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 837\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelative_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;66;03m# Likely running on Windows\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1314] A required privilege is not held by the client: '..\\\\..\\\\blobs\\\\f1f6a44d59f78c8979183439c53d33fddf0dbde5' -> 'C:\\\\Users\\\\dabho/.cache\\\\huggingface\\\\hub\\\\models--facebook--contriever-msmarco\\\\snapshots\\\\abe8c1493371369031bcb1e02acb754cf4e162fa\\\\tokenizer_config.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacebook/contriever-msmarco\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/contriever-msmarco\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere was Marie Curie born?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBorn in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:549\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 549\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    551\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:401\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 401\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\transformers\\utils\\hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    405\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\huggingface_hub\\file_download.py:1218\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(blob_path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_download:\n\u001b[0;32m   1216\u001b[0m     \u001b[38;5;66;03m# we have the blob already, but not the pointer\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating pointer to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, blob_path, pointer_path)\n\u001b[1;32m-> 1218\u001b[0m     \u001b[43m_create_relative_symlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointer_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;66;03m# Prevent parallel downloads of the same file with a lock.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uiuc_chatbot\\lib\\site-packages\\huggingface_hub\\file_download.py:841\u001b[0m, in \u001b[0;36m_create_relative_symlink\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;66;03m# Likely running on Windows\u001b[39;00m\n\u001b[0;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 841\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindows requires Developer Mode to be activated, or to run Python as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man administrator, in order to create symlinks.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIn order to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivate Developer Mode, see this article: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    845\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    846\u001b[0m         )\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Windows requires Developer Mode to be activated, or to run Python as an administrator, in order to create symlinks.\nIn order to activate Developer Mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "\n",
    "sentences = [\n",
    "    \"Where was Marie Curie born?\",\n",
    "    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n",
    "    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eugène Curie, a doctor of French Catholic origin from Alsace.\"\n",
    "]\n",
    "\n",
    "# Apply tokenizer\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Mean pooling\n",
    "def mean_pooling(token_embeddings, mask):\n",
    "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "    return sentence_embeddings\n",
    "embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "612b182cb4c3e0acfd877acc6c10f43d075b0ae43380d6b249d2d2b5490153b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
